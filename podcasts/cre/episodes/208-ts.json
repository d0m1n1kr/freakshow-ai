{
  "transcript": [
    {
      "speaker": "Tim Pritlove",
      "time": "0:01:27",
      "text": "Ihr hört CRE Technik, Kultur, Gesellschaft. Mein Name ist Tim Brittlaf.\nUnd ich sage hallo zu allen, die so lange durchgehalten haben und äh nie die Hoffnung aufgegeben haben, dass diese Serie fortgesetzt wird. Jetzt wird sie fortgesetzt.\nUnd ich kann euch gleich dazu sagen, das wird auch so bleiben, denn äh das hier ist ein Kanal, der nie austrocknen soll und ähm sehr viel mehr gibt's da eigentlich auch nicht zu zu sagen.\nÄhm ja, umso mehr freue ich mich, äh heute mal wieder äh ein neues Thema in Angriff nehmen zu können und ich finde, das passt hier auch ganz gut rein.\nÄhm konkret soll's gehen um Big Data und auch,\num neuronale Netzwerke, sagen wir mal grundsätzlich, um die Frage der Organisation großer Datenmengen,\nUnd ja einerseits wie sich das eigentlich äh technisch alles so verhält und wie man die Sachen in den Griff bekommen kann.\nAber andererseits auch für ein bisschen, welche Fragestellungen sich daraus ableiten lassen und vor allem mit welchen Antworten wir so künftig so zu rechnen haben.\nJa und um darüber zu reden, begrüße ich erstmal meinen Gesprächspartner, nämlich den Ulf. Ulf Schöneberg, hallo.\nJa Ulf, ähm wir kennen uns ja schon äh eine ganze Weile."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:02:56",
      "text": "Das stimmt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:02:58",
      "text": "Nicht, dass wir so auf äh täglicher äh Ebene was miteinander äh zu tun gehabt haben,\nAber äh bist ja auch schon so ein bisschen äh wie soll ich sagen, äh hörst ja auch schon so zu den alten Hasen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:03:12",
      "text": "Im fortgeschrittenen Alter. Wir finden."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:03:14",
      "text": "Das wollte ich jetzt nicht sagen. Alte Hase kann man ja auch schon früher werden\nAber ähm ja weiß nicht, wie lange wann wann bist du so in den äh Computertopf gefallen, was äh ist so dahin, hast du so eine schöne Geschichte, wie du dazu gekommen bist."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:03:30",
      "text": "Ich musste, glaube ich, ein Jahr lang meine Eltern anbetteln, bis sie mir 1undachtzig geschenkt haben und da war ich fünfzehn."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:03:38",
      "text": "Wir reden wahrscheinlich von Anfang der achtziger Jahre so was wie einundachtzig zweiundachtzig."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:03:43",
      "text": "Von dem Zeitraum reden wir ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:03:44",
      "text": "Okay und äh hast du da auch reingekriegt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:03:47",
      "text": "Ich habe einen bekommen, ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:03:48",
      "text": "Mit sagenumwobenen einem Kilo-Fyltraum."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:03:51",
      "text": "Ein Kilobyte Ram und der Speichererweiterung, die sich irakisch selbst äh gelockert und alles vernichtet hat, was man eingetippt hatte."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:04:01",
      "text": "Oh Gott. Ja, ein unglaubliches Gerät. Kleine kleine mini Plasse äh Schatulle mit so Folientastatur."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:04:07",
      "text": "Wir haben grad vorhin im äh Vorgang über Englands England Vermächtnisse für die Computer, denn da ist mein persönliches mein ist für mich derjenige, der der mich hingebracht hat so."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:04:21",
      "text": "Mhm.\nJa, das stimmt, ja. Damals hatte ja äh England für dich noch einen bemerkenswerten, also auch heute noch, aber äh damals halt auch so in diesen frühen Computerzeiten mit eben Sinclaire, ZTK einundachtzig, vor der ZX achtzig, später dann äh vor allem auch der Spektrum dann\nverlief's sich so ein bisschen. Ich glaube, glaube da noch unbedingt Quantum XL oder so und dann war dann irgendwann aber auch die Vormachtstellung von äh Commodore und später natürlich dann auch der äh PCs so stark.\nOder auch Arkorn, äh Risk Machines, was dann so zu armen äh führte, was heute ja noch Relevanz hat. Ja genau, das ist so,\nDer Beitrag der Briten der 80er Jahren noch sehr stark war, während die Deutschen da schon enorm geschwächelt haben und."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:05:03",
      "text": "Enorm sich ausgeruht haben. Sie haben's verschlafen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:05:05",
      "text": "Sie haben's einfach verpennt so irgendwie so nach nach nach so ein Mini-Computern kam dann irgendwie einfach gar nix mehr eigentlich oder habe ich irgendwas vergessen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:05:13",
      "text": "Nee, ich wüsste auch nicht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:05:14",
      "text": "Nee, gibt in keinen deutschen Handcomputer vielleicht noch den Schneider, aber das war ja auch nur ein britischer Computer."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:05:21",
      "text": "Und das kam auch später, ne? Das war durchaus ein paar Jahre später."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:05:25",
      "text": "Okay und ähm bist du dann auch gleich so im ins Programmieren gekommen? Also war das sozusagen, ich meine so sehr viel mehr konnte man ja mit so einem ZDF sich auch nicht tun."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:05:37",
      "text": "Das ist heute so ein bisschen schwer zu verstehen, was man früher gemacht hat mit Computern, so ohne Netzwerk und alleine vor so einem Schwarz-Weiß-Röhren-Fernseher sitzend, das ist ein bisschen, das kann man schwer vermitteln\naber tatsächlich äh habe ich die ganze Zeit programmiert, zuerst im Basic und dann bisschen in Maschinensprache. Was anderes gab es nicht.\nEs gab keine Spiele, hm. Ich habe selbst welche geschrieben."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:06:00",
      "text": "Ja, das war eigentlich auch ein ganz guter Ansporn, aber ist eigentlich auch egal, was die was die Dinger konnten. Es war halt einfach so diese Faszination,\nProcessing, sage ich mal, die einen äh hat nicht schlafen lassen.\nWie ging's dann weiter."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:06:17",
      "text": "Ich habe ähm ich habe das tatsächlich bis zum QL durchgehalten, den den Quantum Lieb Computer und äh ich glaube mit dem dann habe ich."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:06:27",
      "text": "Vorhin XL gesagt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:06:28",
      "text": "Ich weiß es nicht mehr. Ich habe den den den QL."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:06:30",
      "text": "Genau."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:06:34",
      "text": "Und damit ist dann meine meine britische Computervorliebe verstorben und dann äh kam der,\nmit dem dann wohl alle äh oder viele angefangen haben, die nicht beim geblieben sind.\nUnd äh mit dem habe ich tatsächlich mein Mathestudium angefangen und auch weitestgehend bestritten bis zum Schluss. Ohne PC. Ich bin ohne PC durchs Studium gekommen und."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:06:58",
      "text": "Du kannst ein abgeschlossenes Mathematikstudium vorweisen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:07:01",
      "text": "Ich habe einen ich kann ein abgeschlossenes Mathematikstudium vorweisen, in das ich mit dem bestritten habe."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:07:07",
      "text": "Wow, hast du's auch mit Informatik mal probiert?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:07:09",
      "text": "Äh ja ich habe noch drei Semester Informatik an der Humboldt Universität studiert, aber da habe ich nicht abgeschlossen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:07:14",
      "text": "Ja, also du kannst nicht nur ein abgeschlossenes Mathestudium vorweisen, sondern auch noch ein abgebrochenes Informatikstudium, das ist ja wirklich perfekt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:07:21",
      "text": "Genau, ich äh ich kann ich kann zwei zwei erfolgreich bestandene Kurse im Compilerbau äh vorweisen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:07:29",
      "text": "Muss man wahrscheinlich auch gar nicht lernen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:07:31",
      "text": "Nee, da ist es dann vorbei."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:07:34",
      "text": "Okay, also dein Ding war eigentlich schon immer irgendwie das Kohnen. Ja."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:07:38",
      "text": "Ja immer. Das kann man so sagen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:07:43",
      "text": "Und dann kam das Web."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:07:44",
      "text": "Und dann kam das Web ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:07:48",
      "text": "Da bist du doch irgendwie ganz gut unterwegs gewesen, oder? War das nicht so lange Zeit so dein dein Blues oder sehe ich das falsch."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:07:55",
      "text": "Das war so die die Zeit, um die internationale Stadt herum, ne? Da haben wir viel Webkram gemacht, ähm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:08:03",
      "text": "Das musst du noch mal kurz erklären, internationale Stadt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:08:05",
      "text": "Die internationale Stadt ist ein ein Kunstprojekt gewesen ähm,\nEnde der neunziger Jahre, was auf eine auf einer sehr frühen, unfertigen, wackligen Technologie den Leuten Kunst und Internet äh für private Leute anbieten wollte als EV.\nUnd hatte so ein hatte das hatte das Vorbild die der digitalen Stadt aus äh aus den Niederlanden,\nDas haben wir so ein bisschen nachempfunden mit einem\nMehr künstlerischen Überbau und hatten aber sehr früh viele Leute aus dem Netzkunstbereich in Berlin bei uns ansässig und äh haben die haben mit denen zusammen eben so sehr frühen Netzkunstprojekte gemacht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:08:48",
      "text": "Mhm. Hier so Thomax und äh."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:08:51",
      "text": "Thomas Kaumann, ja und Armin ist ja auch ein gemeinsamer Bekannter aus der Zeit."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:08:58",
      "text": "Ja das war war eine ganz interessante Zeit, weil man damals so äh das Web war halt noch sehr frisch,\nUnd ich würde sagen auch so in Nordkreisen hat man auch auch sehr wohl noch äh stark unterschieden zwischen so äh dem Web und was das Netz äh sonst noch so für einen\ntun kann und dann ist da irgendwie so so eine ganz frühe Netzkunstszene äh empor geschossen, die da irgendwas schon äh gesehen hat, was,\nschwer zu vermitteln war teilweise, aber was ich auch so kulturell auch gesellschaftlichen Anspruch auch schon von vornherein verwoben hat, zumindest in Berlin war das der Start."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:09:32",
      "text": "Ja, das war so. Also ich ich habe tatsächlich mit den mit den Netzwerken damals über USCP ähm kommuniziert,\nDann hat mich Thomas äh irgendwann mal zur Seite genommen, hat gemeint, guck dir mal das hier an und habe mir,\nDie Homepage vom Vatikan gezeigt, dem ersten CERN Browser, bei war ein Bild drauf zu sehen und ein Satz stand da drüber und hat gemeint, das ist die Zukunft und ich habe ihn ausgelacht und habe gesagt, was soll das denn? Das kann unmöglich die Zukunft sein."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:10:02",
      "text": "Ja, der Vatikan war relativ früh dabei, ne? Äh erstaunlich, ne? The holy see, ich erinnere mich noch. Ja, krass. Ja, das war noch so die Zeit, wo man so jede neue Webseite, wo so jede neue Webseite äh noch ein totales Ereignis war."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:10:14",
      "text": "Die hat man abgefeiert, ja. Guck mal, was die jetzt machen, da bewegt sich die Schrift und jetzt blinkt es. Wow und das blinkt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:10:21",
      "text": "Aha.\nUnd was hast du dann so ähm ja also gerade ein bisschen abgestritten, was mit dem Web zu tun gehabt zu haben. Äh wollte eigentlich so ein bisschen drauf hinaus, womit du dich technisch so äh auseinandergesetzt hast. Dann."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:10:39",
      "text": "Danach ähm alles Mögliche gemacht, was auch mit Webtechnologien zu tun hatte, aber äh habe viele Jahre lang einen Berliner Internet-Provider danach mitgeleitet und habe natürlich dadurch,\neine ganze Menge mit Webtechnologien zu tun gehabt. Äh auf der administrativen Seite wie auf der Kundenseite, also da all die frühen PHP-Versionen und äh,\nVerschiedenen Bibliotheken, die es da gab und die wir so sehen, haben äh die sind mir begegnet."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:11:13",
      "text": "Aber dein Herz hast du dann nicht verloren."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:11:14",
      "text": "Mein Herz habe ich dann nicht verloren. Das war ein reiner Bruderverb, ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:11:18",
      "text": "Ihr die Nerven."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:11:19",
      "text": "Die Nerven, ja. Na ja, auf ähm auf der anderen Seite des Servicetelefons zu sitzen, das zehrt an den Nerven, das weiß jeder, der das schon mal gemacht hat, ne. Und selbst wenn du da eher im Chefsessel sitzt, ist ja trotzdem an den Nerven."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:11:32",
      "text": "Ja, werde verstehen. So hast du das dann auch mal wieder sein lassen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:11:37",
      "text": "Ja, das habe ich selbst ich habe sieben Jahre durchgehalten. Das ist, glaube ich, meine der längste kontinuierliche Zeitraum äh in meiner in meinem Arbeitsleben. Sieben Jahre lang Internet Provider.\nÄh das hieß früher ein Speedling DE. Das gibt es heute noch. Äh die Marke ist dann verkauft worden. Du kannst dir heute Mäuse kaufen. Die heißen Speedling. Aber das ist ursprünglich mal ein Berliner Internetprovider."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:12:04",
      "text": "Ah okay."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:12:05",
      "text": "Sehr klein, sehr innovativ."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:12:07",
      "text": "Berlin hatte ja ohnehin eine sehr lebendige Internet-Provider-Szene, wo wirklich,\nähm einiges so kulturell entstanden ist, was ihr auch verhältnismäßig lange äh gehalten hat. Und so net MBX und äh all diese ganzen äh Initiativen, die auch extrem früh dabei waren, also es äh."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:12:27",
      "text": "Das muss man sagen, ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:12:27",
      "text": "Nicht nur, dass das die ersten Provider waren, sondern die waren auch an sich sehr schnell im Gesamtgefüge deutsches Internet äh unterwegs."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:12:37",
      "text": "Wir haben tatsächlich sogar im gleichen Gebäude wie gesessen. Hatte ganz oben am äh Breitscheidplatz gesessen und wir in der fünften Etage, wenn ich mich richtig erinnere."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:12:46",
      "text": "Ach so, echt. Okay. Ja. Na gut, aber irgendwann muss man das wahrscheinlich sein lassen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:12:52",
      "text": "Irgendwann muss man das sein lassen,\nUnd jetzt arbeite ich bei einer wunderbaren Firma namens die unbelievable Machine Company und ähm mache das, was gerade,\noder was wohl von wenn man dem Zitat nachgeht.\nIch bin jetzt Data Scientist. Es kommt mir ein bisschen so vor als wenn ich das schon immer bewähre aber es hat jetzt dieses neue Label bekommen ähm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:13:22",
      "text": "Wo kommt denn das Zitat her mit dem der Sexy in das nicht von dir jetzt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:13:25",
      "text": "Ich weise, es kommt nicht von mir, es kommt von irgendjemanden, der wirklich berühmt in dem Umfeld ist und ich ich habe seinen Namen nicht parat. Es ist, glaube ich, auch ein der das äh so zitiert hat."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:13:36",
      "text": "Genau, das hält ich jetzt schon ein paar Jahre gefangen, das Thema."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:13:39",
      "text": "Ja, das hält mich jetzt schon ein paar Jahre gefangen,\nAlso tatsächlich ich äh erzähle die Geschichte immer mhm dass ich Dateience schon gemacht habe so um 2tausend herum. Ich kann mich noch erinnern, dass sich unsere Mailserver administriert habe und es gehasst habe äh,\nUnd natürlich relativ früh der Wunsch, da war, dass man den Müll von dem Nicht-Müll auseinandersortieren kann und könnte, können wollte.\nUnd es gibt da diese wunderbare Technologie namens Spamasin, die man auf Servern zum Einsatz bringt und das ist tatsächlich ein Verfahren des maschinellen Lernens.\nNämlich ein ähm Netzwerk, was man eine ganze Zeit lang trainieren muss auf ähm Müll und nicht Müll und irgendwann kann er selber entscheiden, diese Mail ist eher Spam oder eher nicht Spam.\nDas ist tatsächlich klassisches Maschinenlearning,\nÄhm man bringt einem Computer eine ganze Zeit lang bei, anhand von Beispielen wie Fall A und Fall B aussieht und irgendwann sagt man, okay mein Lieber, ab jetzt bist du selber dran, jetzt entscheidest du bei neun Mails,\nOb das Pal A oder Fall B ist."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:14:47",
      "text": "Finde ich jetzt interessant, dass dass wir jetzt mit Spam kommst, weil wir hatten ja vorhin schon so ein bisschen im Vorfeld äh drüber gerätselt, wo sozusagen diese ganzen äh Fragen, auf die wir heute jetzt äh kommen wollen, so ein bisschen zum Einsatz gekommen sind, also wo sozusagen überhaupt äh das Problem,\ngroßer Datenmengen und ihres äh großer Diffuser-Datenmengen und ihres Verstehens äh herkommt und dabei ist mir der Aspekt Spam äh,\nGar nicht in den Sinn gekommen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:15:14",
      "text": "Und ich hab's komplett vergessen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:15:15",
      "text": "Ja\nUnd äh das stimmt. Und ich frage mich jetzt gerade, wann ging denn das eigentlich los mit diesem Spam? Ich meine, wir saßen ja am Anfang in diesem Internet schon noch so ein bisschen in so einem in so in so einem idealen Ort, nicht. Äh hat uns keiner,\nbelästigt und und der Signal to Noise äh Level,\nAlso jetzt auf so technischer oder auf kommerzieller Ebene der war sozusagen noch wunderbar."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:15:38",
      "text": "Der war wunderbar."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:15:39",
      "text": "Äh weil er hat halt einfach niemand äh belästigt. Natürlich musste man's äh immer noch so mit den anderen Diskutanten im Netz aushalten. Das Problem gibt's heute noch. Aber das das das E-Mail eben so als Marketing äh Belästigungs- und äh Betrugs äh,\näh Instrument entdeckt wurde. Wann ging denn das los? Das war dann."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:15:58",
      "text": "Ich würde sagen, Ende der neunziger."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:16:00",
      "text": "Ich glaube auch so."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:16:01",
      "text": "Da war das das erste Mal so, dass ich kann mich erinnern, dass das Kundenanfragen auf uns reinprasselten als technische äh Sir Level ähm Dienstleister kümmert euch da mal drum, das wird uns zu viel.\nUnd da bin ich auf diesen auf diese ganz frühe Version von dem Spambaczessin, das war glaube null Punkt eins oder null Punkt zwei irgendwas ganz krudes, schrecklich zu installieren.\nModul mit Bibliothekenabhängigkeiten, die auch fürchterlich waren. Ähm da das war das war der Zeitraum. Ich würde sagen so siebenundneunzig,\nDas müssten wir jetzt mal mit Zahlen unterlegen, aber ich."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:16:37",
      "text": "Ja, hast du gut geraten. Also Spamis Hessen ging tatsächlich äh 7undneunzig los und das dürfte dann schon so ein bisschen die Reaktion auf,\nja fünfundneunzig, sechsundneunzig. Das war ja auch so die Zeit, wo überhaupt alle so das Internet äh entdecken, also das weiß ich\nsehr gut, wie dann auf einmal alle anfingen, Internet zu schreien. Ja, hier Internet die Zukunft und so, nachdem ja äh Microsoft vorher noch äh versucht hat, ähm,\nMicrosoft Network und äh andere äh Ideen\nIns Feld zu führen. Alle wollten ja dann auf einmal mit diesen Diensten irgendwie so ihre eigenen äh Communitys und Abhängigkeiten bauen. Microsoft schon sehr klar äh hin in diesen ganzen Payment äh Bereich, also nicht Payment im Sinne von, dass man\nZahlung gemacht, sondern dass man halt einfach darüber kaufen äh konnte,\nApple hatte ja immer so kurz so ein kleines äh verzweifeltes äh äh Netzwerk hier mit EWORLD und vorher gab's natürlich Computer und diese ganzen Mailbox basierten modembasierten äh Netze, die alle noch nicht so richtig Internet waren.\nUnd das Internet, das eigentlich nie so als kommerzieller Dienst geplant war,\nDas war dann auf einmal für alle die Plattform und ich weiß nicht ganz genau, wo was da so der Auslöser war. Ich meine, das war natürlich so ein so ein so ein so ein Prozess über die Zeit, aber das Ganze kulminierte so fünfundneunzig."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:17:55",
      "text": "Ja, 95 ging es richtig los."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:17:57",
      "text": "Ne also also gerade eigentlich als Bill Gates sein äh tolles Buch geschrieben hat\nThe Road a Head und so weiter. Ich weiß, ich hab's nie gelesen und schon gar nicht gepasst, aber äh man sagt dem Buch nach, dass es also äh wenn wenn es das Wort Internet überhaupt drin geführt hat, ging's darum nicht,\nÄhm also so viel zum Thema äh Visionen bei Microso,\nÄhm aber aber dann war dann halt irgendwie das Netz da. Und dann stürmten auf einmal alle ein und dann saß da so ein bisschen dann auf seiner äh idealen Insel äh nicht so einen digitalen Hängematte und dachte sich so, wo kommt denn jetzt hier irgendwie die ganzen Touristen her."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:18:34",
      "text": "Brauchen die eine Führung."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:18:35",
      "text": "Ja so und äh warum schreien die alle so laut? Ja das das war wirklich ein bisschen schwierig äh für uns, aber na ja Gott so ist es halt normal.\nGenau, jetzt hast du schon im Prinzip so ein erstes äh Stichwort äh fallen lassen, nämlich so Machine Learning.\nVielleicht räumen wir's noch mal so ein bisschen von der anderen Seite her auf, ähm,\nWas ich jetzt mal ein bisschen betrachten wollte, ist einfach dieses Problem, dass ähm man,\neben in der elektronischen Daten und Programmierwelt es einfach in zunehmenden Maße mit einerseits großen,\nDatenmengen zu tun hat, wo's ja heute diesen schönen, griffigen Begriff ähm,\nBig Data gibt, ja, wo man sich natürlich gut fragen kann, wann wann ist Delta wirklich äh big, ja? Also,\nWovon hängt das ab? Ist das sozusagen eine eine reine Anzahl von Bites oder ist es eher so die Komplexität? Ähm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:19:37",
      "text": "Das ist eine gute Frage. Ich glaube, je nachdem, welchen Ansatz man wählt, also welche Brille man sich aufsetzt und im Verb danach forscht, wird man ganz unterschiedliche Antworten bekommen. Ähm,\nDie Marketing machen wir jetzt technisch. Wir sind ja Techniker. Ich mache das immer gerne so äh Big Data ist all das, was nicht mehr in deinen Computer passt.\nFür das die Festplatte zu klein ist, wird das dein Prozessor nicht mehr in der Lage ist, das rechtzeitig wegzuschaufeln, dann musst du dir was anderes einfallen lassen. Einen zweiten, einen dritten Computer, mehrere Festplatten im Netzwerk zusammen und dann bist du, glaube ich, im Big Data Bereich.\nDas ist immer noch nicht das Big Data, was Google meint oder was eine Suchmaschine meint, aber."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:20:18",
      "text": "Ist halt relativ. Also in dem Moment, wo man einfach so einen definierten Prozessingraum hat und wenn der quasi von den Datenmengen beziehungsweise den Anforderungen diese zu durchkämmen, überfordert wird, dann wird big. Hm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:20:32",
      "text": "Tatsächlich sind wohl die meisten Fälle von Kunden, die sagen, wir machen jetzt wir müssen jetzt Big Data machen.\nDas sind meistens Sachen, die prima in den Hauptspeicher passen, wenn man sie sich geschickt anguckt, äh aber es gibt eine ganze Menge Fälle, die eben nicht mehr in den Hauptspeicher deines oder meines Computers passen, dann muss man,\nmir was einfallen lassen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:20:51",
      "text": "Mhm. Äh vielleicht nochmal das das Bem Beispiel äh rausnehmen. Das war ja dann sozusagen auch so eine Daten,\nQuelle, mit der man erstmal gar nicht so äh gerechnet hat. Also es\nkamen mir dann mehrere Sachen zusammen. Dieses ganze E-Mail-System ist ja dann eben in auf dieser Insel der glutseligen äh entwickelt worden, Akademiker wollen sich äh Nachrichten schicken und man kam ja auch schon im Design von E-Mail irgendwie auch nicht auf die Idee da\nirgendjemanden nennenswert zu misstrauen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:21:20",
      "text": "Security ist da auf keiner Ebene eingebaut gewesen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:21:23",
      "text": "Oder ja also in irgendeiner Form so ein so Vertrauensebenen noch mit äh einzuziehen oder irgendwie zu berücksichtigen war einfach komplett vergessen worden.\nNachvollziehbar auch, wenn man sich die die Situation damals anschaut, also ich glaube, dass auch niemand dieses Internet auch nur ansatzweise als global Umspannendes Netz, was alle für alles benutzen, in irgendeiner Form angedacht haben."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:21:47",
      "text": "Sicherlich nicht, nee."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:21:49",
      "text": "Und dann kam's halt zu solchen Unfällen und als dann eben diese kommerzielle äh Expolitation von äh des Internets äh begann.\nHatte man halt auch sofort äh einfach die vorherrschenden äh Qualen der normalen kapitalistischen Welt aufm Buckel und das Erste, wo es sich halt so richtig unangenehm,\näußerte Wahrheit Spam so."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:22:11",
      "text": "Genau und das ist tatsächlich vielleicht ein schönes Beispiel für für Big Data jetzt aus aus meiner persönlichen Sicht. Da gab es, es gab kleine Mailserver, mit denen wir angefangen haben und die haben Mail ausgeliefert und natürlich auch mal Kettenbriefe und sind da\nerstickt, aber irgendwann ähm,\nIst das ein zunehmender Punkt des Ärgernisses gewesen, weil die das Ausmaß an Spam so viel stärker ist das Normale als der die normale Mail wurde,\nDass man,\nnicht mehr damit klar kam, ne. Man musste sich irgendwas einfallen lassen, wie man das geschickt aussortiert. Wir haben das von Hand gemacht. Wir haben ganz unterschiedliche Rangehensweisen gehabt. Ich bin dann irgendwann eher aus Zufall auf diesen Spamas hingekommen,\nÄh weil ich wahrscheinlich geschickt gegoogelt habe, gegoogelt habe ich habe ich oder da habe ich zu der Zeit."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:23:00",
      "text": "1997 hast du bestimmt nicht gegoogelt. Nein, nein."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:23:02",
      "text": "Habe ich nicht, ne? Dann habe ich wohl was anderes gemacht. Ich weiß es nicht mehr, was ich habe. Ich habe irgendwo gesucht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:23:08",
      "text": "Ja, also man suchte damals eigentlich bei Alter Avista. Das war so die eigentliche Suchmaschine, wenn man jetzt mal oder was ich mir halt auch vorstellen könnte, dass du so im Usnet gegraben hast, dass man oder vielleicht sogar hast du jemand gefragt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:23:22",
      "text": "Ja das könnte auch, oh ich habe jemanden gefragt, das könnte gut sein. Aber tatsächlich Alter Vista, auch ein schönes Beispiel für Big Data. Ähm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:23:29",
      "text": "Die daran ziemlich zerbrochen sind sogar."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:23:30",
      "text": "Ja, ja, die du einen ganz anderen Ansatz gefahren sind, ne. Wir haben eine einen großen Rechner mit riesigen Hautspeicher und da ist der gesamte Index drin, also nichts verteilt, nichts äh auf hunderte von kleinen Rechnern verteilt, sondern eben einem der große,\nCBU-Ansatz ähm und das hat irgendwann nicht mehr funktioniert, beziehungsweise hat der Ansatz den Google gefahren, hat äh eben sehr viel besser funktioniert, auch vom Algorithmischen her, aber jetzt vom Technischen, ne."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:24:00",
      "text": "Ja, lass uns erst mal beim Spam bleiben. Dann kommen wir gleich nochmal auf Google."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:24:04",
      "text": "Dass bei mir, wie hat man ihn wegsortiert?\nAlso erst mal hat der Rechner jetzt erst mal ist der kleine, ist der kleine Rechner zunehmend ein großer Rechner geworden durch mehrere Iterationen bis es am Ende glaube ich eine eigene Sun 450 war. Das war ein relativ dicker Vogel zu der Zeit, der sich nur mit Mail beschäftigt hat.\nAuch deren Netzwerkanschluss ist bestätigt gewachsen in seiner Kapazität.\nDas war zum Schluss eine zwei Gigabit-Leitung, nur für Mail. Das ist für die Zeit auch was richtig, richtig was richtig viele gewesen, ne. Und ähm,\nDahinter muss da eine Technologie sein, die die Mails,\nschnell genug wegsortiert in die richtigen Ordner sortiert oder eben auch unterscheidet, Spam oder nicht, Spam. Das ist sehr frühes Big Data gewesen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:24:50",
      "text": "Und wie hat das gemacht?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:24:52",
      "text": "Der Spamazessin hat das gemacht, ohje, ich habe mich nicht vorbereitet. Jetzt ähm das wird jetzt ins Unreiben und das wird ein Ritt auf dünnen Eis. Ich denke, ähm es ist ein,\nBei easy Network wie ihr es wie ihr's macht oder gemacht hat äh zu der Zeit. Ähm ist es, ne?"
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:25:14",
      "text": "Ja, dafür war's bekannt, ja. Mhm. Mhm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:25:15",
      "text": "Dafür war's bekannt. Das ist ein sehr einfaches, aber sehr, sehr gut funktionierendes äh Verfahren des maschinellen Lernens. Äh."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:25:22",
      "text": "Also unter anderem, es waren natürlich so mehrere Methoden, die da drin äh zum Einsatz kamen, aber gerade dieser also da da habe ich überhaupt von diesem Basefilter überhaupt das erste Mal gehört in dem Zusammenhang."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:25:34",
      "text": "Es ist es ist wie gesagt ein sehr sehr einfaches, aber ganz wunderbar funktionierendes grundsolides Verfahren, deswegen haben die das auch gewählt, ähm was schnell lernt ähm,\nund äh eben noch schnell zum Einsatz kommen kann. Ähm,\nUnd das man man muss es anlernen zu unterscheiden, indem man ihm genug negative und positive Beispiele gibt,\näh er fängt dann an nach statistischen Methoden bestimmte Koeffizienten in äh irgendwelchen Gleichungen äh so hin und her zu legen, dass er bei einer neuen, unbekannten Nachricht automatisch entscheiden kann.\nIch setze den Score jetzt auf null Komma sechs, 0,7 für Spam und damit ist es eben sehr viel wahrscheinlicher, Spam als äh eine gute Nachricht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:26:18",
      "text": "Das heißt, also das ist im Prinzip so eine einfache Form von Machine Learn Learning, also ein Algorithmus versucht ein Wissen,\nzu schaffen über eine Verhältnismäßig diffuse äh Datenmenge, weil es ja,\nsehr unterschiedliche ähm,\nsehr unterschiedliche Parameter sind, dieser Datenwolke, die einem indiz für äh ins Kröpfchen oder ins Töpfchen äh gibt.\nWas macht denn ein Basefilter genau? Also was ist denn da so äh der."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:26:57",
      "text": "Man könnte vielleicht erstmal einen Schritt vorher rangehen und sagen, was macht man, wie unterscheidet man, äh ob eine Nachricht spam oder nicht, Spam ist, ähm,\nals Mensch fliegt man da irgendwie mit den Augen rüber und erkennt nach wenigen Hin- und Herschwenken mit den Pupillen, okay das sieht nach Spam aus äh oder eben nicht, aber in diesem Prozess mit dem mit den Augen darüber fahren passiert in unserem Gehirn verhältnismäßig viel.\nWas man Maschinen relativ schwierig beibringen kann. Man hatte eigentlich keine richtige Handhabe dafür,\nAnsatz könnte sein Wörter zu zählen äh und eine Liste mit schrecklichen Wörtern zu haben, die eben eher im Spam vorkommen oder eine Worte mit positiven Beispielen, die eher in ähm,\nIn guten Beispielen vorkommen, man hätte vielleicht auch noch eine Liste mit Wörtern, die total irrelevant sind, wie hallo, Komma, äh den, der ist und so weiter, also sogenannte Stopbirds äh.\nOder man könnte auf die relativ komplexe Idee kommen äh Wörter als Mehrfach-Touple zu begreifen, also sich so Sequenzen von zwei oder drei Wörtern anzugucken. Andrums nennt man das.\nUnd äh dann diese in die gleichen äh Töpfe zu werfen, gut, schlecht, nicht so gut äh,\neben durch die Mail durchzugehen, diese zu zählen und nach einem gewissen Algorithmus zu sagen, okay so und so viel aus dem Bereich, so und so viel aus dem anderen Bereich. Das Ding ist eine Spam oder das ist eben eine positive,\nmailen ähm was man,\nmachen könnte, wenn man das äh noch ein bisschen elaborierter angeht, äh dass man sagt, der Algorithmus produziert diese ähm packt das in so Vergleichstöpfe,\nProjiziert. Die ganze Mail oder baut aus diesen ähm einen Vektor zusammen, wo jedes Element des Vektors so ein ist,\nUnd hat dann einen hochdimensionalen Vektorraum. Jeder jede Mail, die man liest, ist Teil von diesem Vektorraum dann. Ist ein Punkt in diesem hochdimensionalen Vektorraum,\nUnd man kann in diesem hochdimensionalen Vektorraum, genau wie auf einem Blatt Papier, äh Kreise ziehen, eben komplexe indimensionale Kugeln,\nDefinieren äh oder Rauminhalte und in diesem Rauminhalt äh ist eher der Spam zu Hause und in dem anderen Rauminhalt ist dann eher der Nichtspam."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:29:21",
      "text": "Hochdimensional heißt schon mehr als drei oder?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:29:24",
      "text": "Ja, so viel, also wenn du 30 hast, dann ist er 30dimensional und."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:29:29",
      "text": "Okay, das lässt sich ja erfahrungsgemäß immer nicht so so leicht vorstellen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:29:32",
      "text": "Nee, das lässt sich richtig schwierig."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:29:33",
      "text": "Aber wenn man sich's einfach mal nur versucht mit drei Dimensionen vorzustellen, dann habe ich im Prinzip so einen dreidimensionalen Raum, wo ich so bestimmte Seifenblasen herumschweben habe und auf der einen Seitenblase steht drauf, hier ist äh wohl,\nHier wie Spam und äh in anderen so hier ist wohl eher okay. Auch wenn das nicht genug Dimensionen sind, um jetzt wirklich dem Spam äh,\nHerr zu werden, aber es ist vielleicht jetzt genug Dimensionen sind, um sich das mal so vorzustellen. Also man man man mappt sozusagen bestimmte Teilbereiche eines,\ndieses Raums, wieviel Dimensionen er auch immer haben mag, als gut und böse und dann verortet man das anhand dieser ganzen Untersuchung, ist er da, ist er da und dann hat man seine Aussage, verstehe ich das richtig?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:30:16",
      "text": "Genauso würde ich da sagen, ja. Das ist perfekt erklärt. Ähm und der,\nJa und das maschinelle Verfahren äh verschiebt eben genau diese diese Grenzen in diesem Raum, ob das nun in zwei oder dreidimensionalen ist. Man ich erkläre das immer am liebsten, indem ich mir ein ein,\nBlatt Papier nehme und ein Koordinatensystem draufmale, weil das funktioniert im zweidimensionalen ganz genau so.\nDu hast Punkte im zweidimensionalen und irgendwo versuchst du jetzt eine Linie, eine krumme Kurve so durch die Punkte zu legen, dass du die schlechten von den guten Punkten separierst. Das kann irgendwas äh beliebig\nkomplexes sein, je nach maschinellen Verfahren ist das entweder eine Gerade oder eine Kurve oder einen Kreis oder und dann äh hast du die verschiedenen maschinellen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:31:01",
      "text": "Idealfall eine eine leicht mathematisch zu beschreibende äh Kurve, sodass man eben schnell ausrechnen kann, ist das jetzt da drin oder da drüber oder dadrunter? Mhm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:31:10",
      "text": "Genau. Das wünscht man sich nur irgendwas einfach Berechenbares, damit der Computer relativ schnell eine Antwort finden kann, wenn er eine neue Frage gestellt bekommt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:31:17",
      "text": "Also der Spamazessin hatte ja damals auch wirklich schon eine große Wirkung, oder? Also das hat ja schon ganz gut funktioniert."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:31:25",
      "text": "Das hat die Sache spürbar erleichtert,\nAlso man hat äh ich weiß nicht mehr, wie ich den Score eingestellt hatte. Ich glaube, sehr konservativ. Ich habe gesagt, es muss weit über null Punkt acht sein, bevor ich sage, das ist sicher Spam und dann hat man den,\nSpam Score auch glaube ich nur in den Mailheader geschrieben und hat das den Mail Clients überlassen, selbst damit umzugehen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:31:44",
      "text": "Es war mehr nur so ein Hinweis. Mit hier sieht aus wie mit der und der Wahrscheinlichkeit ist das jetzt Bam und dann kann man das immer noch selber machen, weil Problem war ja natürlich, wenn man den Leuten ihre legitime Mail äh entfernt hat, dann äh finden sie das noch schlimmer, als wenn sie zu Spam bekommen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:31:59",
      "text": "Da war sie sauer und zwar zurecht, ne? Er möchte nicht von Maschinen. Man man möchte nicht von Maschinen bevormundet werden. Das werden wir heute sind wir dran gewöhnt, aber zu der Zeit äh äh war das noch ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:32:08",
      "text": "Regte sich noch Widerstand."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:32:15",
      "text": "Ne klar, also ich glaube, wir haben wir haben den Netzwerktreffic mit in dem wir nur die null achter Sachen,\nNee, mhm ich weiß es nicht. Wir haben glaube ich fast dreißig, 40 Prozent des Netzwerktreffics äh gehörig,\nrunterschrauben können, indem wir schon die Sachen, die,\nden Score größer als null Punkt 8 haben, die habe ich sowieso direkt rejected, genau, die habe ich gar nicht angenommen.\nUnd damit kamen sie nur bis zum zum Eingangsnetz Interface von diesem von diesem Mailrechner und wurden ansonsten wieder zurückgesandt und die Welt sollte sich darum kümmern. Aber es hat nicht mehr unseren Rechner belastet. Also das hat wirklich das hat was das hat bemerkenswerte Auswirkungen gehabt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:33:01",
      "text": "Ja, aber Spam ist nach wie vor so ein bisschen das Nahos-Problem der äh,\nDas der der Datennetzer so, das nimmt irgendwie kein Ende und äh geht immer wieder hin und her und ist auch bis heute nicht äh gelöst, wo man sich halt auch wirklich fragen möchte,\nLässt sich das überhaupt noch lösen. Da können wir dann vielleicht ganz am Ende nochmal eine Aussage äh zu äh machen oder zumindest versuchen.\nJa ähm also grundsätzlich im im äh Computing,\nNicht jetzt nur mit dem Aufkommen des des Netzes und und Spam, aber äh Computer wurden immer,\nschneller datenspeicher wurden immer größer, aber wenn man sich sagen wir mal jetzt die Fortschritte in den 70er und 80er Jahren anschaut, dann wirkt das ja aus heutiger Sicht doch,\ngeradezu albern, ja erinnern uns alle immer so an diese Fotos, wenn dann so was weiß ich so aus dem fünfziger, 60er Jahren äh,\nvon IBM eine fünf Megabyte hart ist mit einem Gabelstapler in so einem Flugzeug eingeladen wird. Ja unglaubliche äh Speichermengen.\nAber du sagtest ja schon, Big Data ist relativ und in dem Moment, wo einfach das Computing-Equipment und die Speichermethoden über äh Schritten sind, dann dann muss man sich schon enorm was einfallen lassen.\nWas waren denn so die ersten Forschungs ähm Bereiche, wo man auch wirklich schon sehr früh von von Big Data Problemen,\nHat äh sprechen können."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:34:36",
      "text": "Du meinst Sachen, die nicht die die keinen militärischen Hintergrund haben über die."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:34:42",
      "text": "Na ja, nee, ich will das jetzt gar nicht mal ausschließen. Also ähm,\nIch meine, auf der einen Seite äh also wir haben uns ja von solchen Sachen dann abschrecken lassen. Also man man hat ja sozusagen dann äh ja nee, keine Ahnung, absehbar, dass sowohl CPU, Speicher, äh Hauptspeicher et cetera nicht ausreicht. Das ist etwas, dem kann ich mich nicht\nNicht annehmen. Wir waren nicht in der Lage äh aufm C 64 HD Videos äh auch nur ansatzweise auch nur zu erträumen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:35:08",
      "text": "Nee, das war unvorstellbar."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:35:09",
      "text": "Ja das dann lässt man das halt auch komplett sein, aber auf der anderen Seite gab's ja auch immer wieder Anforderungen. Vielleicht ist Militär ein Beispiel.\nWo man gesagt hat, ja scheiß drauf, äh wir kippen da jetzt einfach so viel Geld äh drauf oder eben Forscher bemühen, weil die Lösung dieser Aufgabe für uns noch sehr viel mehr äh Wert darstellt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:35:30",
      "text": "Ja, ich glaube, wir hatten im im Vorgespräch haben wir über Wetterdaten geredet, aber mir ist gerade eingefallen, ähm,\nWo wir jetzt schon so in den in den in den im geschichtlichen Keller des Internets die ganze Zeit rumkriechen, äh müssen wir auf jeden Fall auch das Zerren äh erwähnen, weil ich meine, das ist der klassische Ort für Big Data, wenn du,\nso viel Sensoren in große Maschinen steckst, heute sind es unglaublich viele, dass Terabyte pro Stunde äh erzeugt werden, aber zu der Zeit waren es eben wahrscheinlich Gigabyte pro Tag und das war zu der Zeit auch big Data, ne?"
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:35:59",
      "text": "Also CERN, dass Atomforschungs äh Zentrum in der Schweiz, was ja auch heute noch mit seinen Larch Hedron Colada äh zurecht News macht.\nWar damals schon ein Beschleunigering und hat versucht, so viel Daten aus diesen ganzen äh Partikelbeschuss und Beschleunigung äh zu generieren, wie es nur irgendwie möglich war und auch das musste halt in irgendeiner Form,\ngespeichert werden."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:36:23",
      "text": "Auch das muss in Speichel muss irgendwo gespeichert werden und das war zu der Zeit mit Sicherheit eine eine lupenreine Big Data Technologie."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:36:32",
      "text": "Und die haben sich ja auch dann äh ordentlich Geräte rangeschafft und auch vor allem früh auch schon äh Netzwerke."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:36:38",
      "text": "Und haben mit den Netzwerken Probleme in der Kommunikation gehabt untereinander und haben dann kurzerhand auch das Verb miterfunden beziehungsweise den Webbrowser, ne."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:36:46",
      "text": "Mhm. Richtig. Und sich die äh Hacker eingetreten. An der Stelle kann ich gleich mal äh auf eine alte Sendung äh verweisen, nämlich die Jubiläumsausgabe, die hundert,\ndas Internet und die Hacker, wo äh Hans Hübner hier lustig erzählt hat von seinen damaligen Ausflügen eben in das äh Netzwerk, was es äh damals\ngemeinhin als HK-Fahrschule bezeichnet wurde."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:37:08",
      "text": "Weil die scheunen Tore weit offen war."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:37:10",
      "text": "Da war die Scheunentore ausreichend offen. Da gab's genug zu haben. Da da traf man sich dann halt so ein bisschen. Das war so die äh die Community, äh die sich da die frühe globale Community sich da äh getroffen hat. Da hatte man dann die richtigen Voraussetzungen.\nDank Big Data, ja."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:37:24",
      "text": "Und genug Spielzeug, keine Frage."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:37:26",
      "text": "Mhm. Aber wie haben die denn das gelöst."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:37:29",
      "text": "Das weiß ich nicht, Tim. Das weiß ich nicht. Ich habe das jetzt einfach mal zurück projiziert und bin der Meinung, das muss so gewesen sein. Ich habe keine Ahnung. Also,\nDie haben äh mit Sicherheit Workstations gehabt. Die werden Großrechner gehabt haben. Sie werden auch schon einen kleinen Serverarchitekturen gehabt haben. Ähm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:37:48",
      "text": "Ich meine, die haben ja vor allem erstmal das Problem des Sammelns an sich. So, ne?\nDaten fallen an, Sensoren generieren diese äh Daten und man will sie ja nicht verlieren. Äh das ist ja der der erste Schritt. Da braucht man einfach erst mal schon mal Heavy Metal.\nDie eigentliche Frage ist ja später,\nNow we have the data watch what we do with it, also äh diese Daten dann eben auch wirklich zu zerstochern und daraus eben auch nennenswerte Analysen zu machen. Ist ja noch mal eine ganz andere Frage."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:38:18",
      "text": "Ja, das ist eine,\nDas ist auch wieder eine eine also das ist auf jeden Fall ist das ein sind das Thema ist das ein Thema des maschinellen Lernens? Weil des Machine Learnings, weil du auf diesen Datenmengen ähm,\nDu willst ja eine Aussage über die Zukunft treffen oder du willst eine,\nTheorie, die du im Kopf hast, bestätigt wissen, also du willst etwas nachrechnen, was diese Sensordaten bestätigen sollen oder du willst etwas in die Zukunft projizieren, ähm aus dem, was du schon verstanden hast. Das heißt, du gehst durch diese riesigen Datenmengen durch,\nMit deinem Verfahren ähm,\nKeine Ahnung, was die da nun wirklich äh benutzt haben zu der Zeit mit Sensordaten von Beschleunigerinnen habe ich noch nie zu tun gehabt. Ich äh kann da jetzt wirklich nur raten, aber."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:39:05",
      "text": "Aber ist ja ein schönes Beispiel. Also hatte das jetzt auch nicht mehr so richtig aufm Zeiger, aber stimmt, klar, ne. Also da in dieser Partikelwelt, also generell viel natürlich bei wissenschaftlichen Experimenten äh,\nimmer schon viel Daten an und letztlich ist ja auch das Internet am Ende auch deswegen,\ngegründet worden, um Daten auch übermitteln zu können, Austausch und akademischen Austausch zu ermöglichen und äh Forschung auch damit auf ganz andere Beine zu stellen. Da war das Zern halt äh nur ein Teil, wenn auch ein sehr wichtiger, großer,\nUnd auch äh sehr erfolgreicher Teil. Ähm aber eben beileibe nicht der Einzige."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:39:42",
      "text": "Nee, es ist irgendwann ähm,\nIm letzten Jahr, im Ende des letzten Jahrtausends, ist den meisten Leuten klar geworden, dass äh große Bereiche der Wissenschaft, der experimentellen Wissenschaft mit massiven Datenaufkommen einhergehen werden und dass sich das auch nicht mehr verändern wird.\nDas ähm dass man diese Daten irgendwo zusammenführen muss, dass man interoperabalität haben muss, äh dass man sie sammeln muss und dass man,\nDen Kollegen, wenn man eine Theorie hat und äh in dem Bereich äh jetzt etwas dünn ist und möchte etwas überprüfen.\nDann muss man das schlicht und einfach einfach mal schnell dahin äh senden können und das sollte vielleicht in endlicher Zeit passieren und nicht über Kontinentalgrenzen mit Post und äh\nnun schneller Austausch."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:40:27",
      "text": "Also auch die Erforschung ist äh Universums, Sternenkarten et cetera. Also ich meine, es gibt ja genug."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:40:33",
      "text": "Die Astrophysiker, die haben äh auch schon sehr früh ihre Maschinen zusammengestellt, ne, um zusammengeschlossen, um sie gemeinsam synchron und Problemen berechnen zu lassen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:40:43",
      "text": "Wobei,\nDas Verständnis, dass so eine Forschung so erforderlich ist, das ist ja noch mal so ein Problem für sich, der der der Wissenschaft diese Relevanz immer wieder neu zu vermitteln. Es gibt einen Teil der Politik, der das sehr wohl versteht. Es gibt äh andere ähm,\npopulärere, orientierte Strömungen, die äh dem auch gerne mal ein Fragezeichen vorsetzen, aber wo du schon angesprochen hast\nDas Wetter, glaube ich, äh das interessiert ja alle. Also ne, wenn man nichts zu sagen hat, dann unterhält man sich halt übers Wetter.\nUnd wie es äh wird und ich erinnere mich noch so in den 80er Jahren war ja im Prinzip die das Satellitenbild von Europa, also Meteosat.\nDas war ja irgendwie so,\nDarüber nachgedacht, aber so gefühlt war das so mit so das das akademischste, was man so täglich vor die Nase gehalten bekommen hat."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:41:43",
      "text": "Stimmt in der Tagesschau so eine Art wissenschaftliches Diagramm, ne. Das ist eher die Ausnahme gewesen. Mhm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:41:48",
      "text": "Ja hier die Kaltfront und so mit so Linien und Dreiecken und so, das war ja dann auch alles noch äh äh sehr einfach und irgendwann kam ja dann auch wirklich eben dieses Foto der Welt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:42:01",
      "text": "Stimmt das Biwetterkarte war traditionell immer gezeichnet und irgendwann kam dieses Satellitenbild dazu, wurde zumindestens eingeblendet, ne? Kann man das noch, findet man das noch irgendwo?\nDieses Bild, ja?"
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:42:12",
      "text": "Ja bestimmt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:42:14",
      "text": "Auf jeden Fall sind die Meteorologen sozusagen die Data Scientists äh der der ersten Stunde, ne, die haben schon ewig äh große Datenmengen gehabt und wollten ihr Modell mit,\nMit diesen Daten unterstützen, um eben Aussagen über die Zukunft zu machen. Also klassisch äh dieselbe dasselbe wie beim du hast ein Verfahren, das hast du etabliert,\nGips-Team, Daten,\nZum Lernen die aktuellen Wetterdaten und vielleicht die Wetterdaten von einer Stunde und von letzter Woche und dann fragst du genau dieses Verfahren und wie wird das Wetter morgen?\nUnd dann kriegst du bestimmte Aussagen mit bestimmten Wahrscheinlichkeiten. Das ist ähm die Vereinfachung ist jetzt ein bisschen sehr stark, aber das ist nichts anderes als das Sinn auch macht, ne? Er sagt auf einer neuen Nachricht,\nAufgrund meiner Daten, die ich habe, ist das äh Spam beziehungsweise morgen regnet's."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:43:10",
      "text": "Und eigentlich ist das ja alles nur Statistik."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:43:12",
      "text": "Eigentlich ist das mehr oder weniger elaborierte Statistik, ja? Es gibt das meiste könnte man auf Statistik zurückbrechen. Es gibt ein paar Sachen, die sind explizit nicht Statistik, aber das sind äh das sind die sind das sind die Wenigeren.\nDie meisten Verfahren aus der Werkzeugbox des maschinellen maschinellen Lernens sind statistische Verfahren."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:43:33",
      "text": "Und trägt das weit, also."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:43:35",
      "text": "Ja, das das trägt relativ weit,\nWenn man die Statistik gut versteht äh und den den Raum, den man äh über den man Aussagen treffen will, wenn man den vorher gut versteht, mit Domainwissen und die,\nDie Features, die man aus diesem aus diesem Raum, äh wenn man die gut enginiert, dann äh trägt das sehr, sehr weit. Und auch schon ewig,\nAlso die Wettervorhersage hat sich in ihrer Struktur, ist jetzt auch dünnes Eis. Ich weiß nicht, was die letzten zehn Jahre gemacht haben, aber ich glaube, so richtig\nganz doll viel ist da algorithmisch nicht passiert. Das wird vermutlich immer dasselbe sein, nur dass sie mit viel mehr Stützstellen arbeiten und das ein oder andere an ihrem Algorithmus einfach verbessert haben. Ähm,\nUnd natürlich die Rechenpower größer geworden ist, also das Modell kann deutlich komplexer sein, was dahinter ist.\nAber,\nich weiß es nicht genau, dass mit den Wetterdaten,\naber doch das äh das trägt ziemlich weit."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:44:41",
      "text": "Jetzt ist ja das Speichern von Daten ein Klassiker. Ich weiß noch,\nals ich das erste Mal in meinem Leben das Wort Datenbank gehört habe, da ähm ging's, glaube ich, um.\nAdressen abspeichern. War ja so ein Homecomputerzeiten noch eine,\neine unglaubliche Herausforderung, aber sozusagen überhaupt dieser Begriff Datenbank,\nDer sozusagen auch schon so implizit äh impliziert, dass so für deine Daten gibt's einen extra Ort. Ja, also so wie wie dein Geld liegt halt auf der Bank.\nGut, der englische Begriff gibt's jetzt eigentlich nicht so sehr. Ja, aus dem Deutschen kann man das irgendwie sehr schön rauslesen, so so ja Daten, das ist halt einfach mal was spezielles. Äh da brauchst du halt jetzt äh äh deinen deinen speziellen Speicherort.\nUnd ja ähm nicht nur, dass die Daten halt alle so für sich erstmal zusammengepfercht werden, sondern als äh Mehrwert.\nOrganisieren wir dir die Daten halt auch gleich, damit eben der Zugriff darauf möglichst effizient ist, also effizient, vor allem natürlich im Bezug auf Geschwindigkeit beziehungsweise, dass man überhaupt\näh dann eben auch wie du schon sagst äh in endlicher Zeit.\nDie Chance hat, an die Informationen heranzukommen, die man ihnen jetzt gerne haben will. Und.\nIn den ja geschichtlich auch grad nicht so ganz Firmen, aber äh,\nDie große Erfindung des zwanzig Jahrhunderts in dieser Hinsicht war die Relationale Datenbank. Die Idee, dass man eben,\nDaten in Tabellen führt, die äh in sich gleichförmig sind, davon,\nmehrere äh auch äh nebeneinanderstellen kann und zueinander in Relation setzen kann. Und diese Relationen\ndem Datenbanksystem selber auch äh bekannt sind und Zugriffe darauf ermöglicht,\nja die dann eben schnell automatisiert ablaufen können und in dem Moment, wo ich neue Daten reintanke, werden eben diese Relationen auch äh immer entsprechend ähm aktualisiert, dass man eben gut drauf zugreifen kann.\nUnd alle waren glücklich und hielten die Hände hoch und sagten mit relationellen Datenbanken gehen wir äh in die Zukunft und erforschen das Universum und alles wird gut."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:47:13",
      "text": "Ich glaube, das ist auch äh das können sie sich auch,\nDas das ist jetzt nicht falsch, ne? Das äh hat eine ganze Zeit lang gut gehalten, das Paradigma. Hält auch immer noch, also mit relationalen Datenbanken, das ist."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:47:25",
      "text": "Wird das meiste gefahren."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:47:26",
      "text": "Wird das meiste gefahren, würde ich auch sagen. Also klar haben wir jetzt die ganzen äh no SQL Geschichten und die sind natürlich auch sehr, sehr schick und äh schön, aber so eine grundsolide, relationale Datenbank äh,\nMacht, glaube ich, Oracle immer noch reich und äh bringt Leute dazu, auf äh großen Segelschiffen, äh Regatten fahren zu können und Ähnliches."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:47:48",
      "text": "Ja, das stimmt.\nWobei ich mich halt frage, ist das jetzt nur so, weil die sozusagen ihren Markt so fest äh im Griff haben und die Leute halt äh Veränderungen äh nicht mögen oder ob halt tatsächlich ähm äh\nDas Versprechen der relationalen Datenbanken äh auch immer noch genug eingelöst wird und auch alle wirklich den Benefit davon haben.\nObwohl man's auch eigentlich ja hätte an ganz anders angehen können."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:48:11",
      "text": "Also ich glaube, man kann es anders rangehen. Man kann anders rangehen, das kann man immer ähm das zeigt, glaube ich, die Entwicklung der Menschheit. Ähm aber lationale Datenbanken funktionieren gut, die sind grundsolide verstanden in, glaube ich, fast jedem, jedem Aspekt äh so,\ngroße Datenbanknamen ähm versprechen eben auch, dass das Problem in jedem Fall gelöst wird. Das gehört sozusagen zum Firmenversprechen.\nDeswegen kriegst du ja bei jedem bei jeder Entscheidung, welches Produkt nun gekauft werden muss, kommt irgendwann das große O. Ne, dann wissen wir, was wir haben, das funktioniert auf jeden Fall. Das ist bei unseren Konkurrenten auch so, was auch immer da an Begründungen kommt. Ähm,\nGenau."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:48:54",
      "text": "Ja. Aber was machen denn diese relationalen Datenbanken jetzt wirklich für einen? Also was ist denn da sozusagen die innere Rente äh Magie?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:49:04",
      "text": "Ja, die lassen dich mit dieser wunderbaren Abfragesprache eben aus der großen Datentonne deine deine Daten, die du haben möchtest, ähm,\nFiltern sie heraus, ne? Das ist das, was sie schön macht oder was sie schön machen. Ähm,\nAber im Grunde, ich nenne es deswegen immer nur Datentonne. Ich äh es ist eine große Tonne und äh,\nJedes jedes Datentütchen hat äh unterschiedliche Flex und,\nmit der relationalen Datenbank äh Abfragesprache stellst du diese Datentütchen neu zusammen und legst sie in eine, dir gefallende Reihe. Mehr ist es nicht. Ich würde jetzt,\nIch bin kein kein großer Datenbank-Evangelisator, dass ich betrachte das immer nur als klar, da gibt es äh tausend Sachen, die man,\nbeachten muss, wie Geschwindigkeitsskalierbarkeit äh Replizierbarkeit und ähm im kommerziellen Umfeld will man natürlich auch ähm,\nSachen haben, äh wie absolute Transaktionssicherheit und ähnliches und das bieten dir,\nDie meisten modernen revolutionalen Datenbanken einfach äh von Haus aus. Hm.\nDas Schlimme an den relationalen Datenbanken ist das, was sie so gut macht. Das sind nämlich die Relationen. Du musst äh in eine um in einer relationalen Datenbankdaten anzulegen, musst du dir vorher eben sehr genau überlegen, was will ich tun, in welchen Topf, wie will ich die Sachen,\nverknüpfen, das heißt du musst diese sogenannten normalen Formen äh,\nDurch exerzieren, das heißt nichts anderes als die Tabellenstruktur äh wie eine Tabelle auf die andere zeigt, äh wir überlegen,\nund da fiel Energie reinbringen und das macht sie am Ende auch ein bisschen sperrig, weil wenn du dir das einmal überlegt hast und dann ändern sich deine Anforderungen an die Datenbank, dann musst du,\nDiese Definition neu überlegen und auch neu durchführen. Das heißt, du musst durch diese normalen Formen wieder durch und musst\nÄh die Daten in neue Töpfe legen, umsortieren. Das ist alles sehr aufwendig und äh bisschen ärgerlich, wenn man Anforderungen hat oder wenn man Business Cases hat oder Cases überhaupt äh,\nWo man weiß, das ändert sich jetzt öfter mal. Und so sind wir zu den schemafreien Datenbanken gekommen, die heute sehr erfolgreich sind.\nDie sogenannten No SQL Datenbanken. Mit den ganzen Namen, die da so über die Bühne,\nLaufen wie Mongo, DB und Radis und Elastik und es gibt 30 verschiedene Couch-DBY."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:51:42",
      "text": "Ja ja zum Beispiel auch schon mal äh ein Thema. Ähm wo wir das auch schon so ein bisschen äh angegangen hatten. Ich wollte jetzt aber noch mal so ein bisschen die Realität von den von diesen Datenbanken verstehen, weil im Prinzip ähm,\nDas, was ähm die Datenbanken ja dann ausmacht, ist eben dieser dieser Index. Also,\nDie Idee, dass man in dem Moment, wo Daten neu in das System reingeworfen werden auf Basis der existierenden,\nVorher definierten Schemata, also dieser Relationen, in denen die Daten stehen und damit quasi der inneren ähm,\nVorformulierung, was man denn gerne später schnell abrufen äh möchte, dass halt diese Indizes angelegt werden."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:52:30",
      "text": "Ja, die relationale Datenbank\nBietet dir ja, du kannst äh spalten und ähm Tabellen definieren und kannst die dann ganz normal abfragen. Wenn du allerdings,\nAuf die Geschwindigkeit achtest, also wenn deine Antwort schnell kommen soll, dann kannst du in den allermeisten Datenbank wahrscheinlich wirklich in allen nicht,\nWill mich jetzt nicht versteigen, aber ich denke, es wird in allen so sein. Kannst du sagen, okay, diese Spalte oder diese Tabelle, die wird jetzt vom Typ Index.\nGeht die Datenbank hin mit ihrem eigenen Mechanismus und fängt an alles, was in dieser Tabelle oder in der Spalte drin ist,\nin ein spezielles Format zu bringen. Das ist oftmals ein invertierter Index ähm,\nUnd ähm macht bestimmte, verpointert das in einer geschickten Art und Weise, sodass du, wenn du die Frage stellst, die Antwort äh hundertmal so schnell kriegst wie vorher."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:53:23",
      "text": "Weil sie im Prinzip schon vorgekaut ist."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:53:25",
      "text": "Sie ist vorgekaut. Er hat alle, alle möglichen Fragen im Prinzip schon äh einmal durchgerechnet und ihn als Antwort in diesen Index gepackt. Das ist jetzt bisschen sehr vereinfacht, aber im Grunde kann man das so sagen.\nJa und dieser Index hat dann den Nachteil, dass wenn neue Daten reinkommen, dann muss der Index jedes Mal neu berechnet werden.\nJe nach Indextyp äh ist das entweder eine Sache, die sehr schnell oder sehr langsam kommt. Die meisten Indices haben die dumme Eigenschaft, dass wenn du,\nDreißig, 40 Prozent neue Daten drin hast, äh dass der Index einmal komplett neu angelegt werden muss. Und jeder, der großen Index angelegt hat, der weiß, das dauert einfach manchmal eine ganze Nacht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:54:05",
      "text": "Warum muss der Ganze angelegt werden, wenn du 30 Prozent Daten dazukommen?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:54:09",
      "text": "Ja irgendwann reicht diese reicht diese geht dann über so viele ähm Umwege, dass sich das einfach nicht mehr lohnt. Das ist nicht mehr schnell genug."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:54:17",
      "text": "Ach so, weil wenn da nachträglich hinzugefügt werden, muss quasi immer noch über überall was rangehängt werden\nUnd was auf was zeigt, was auf was zeigt, was auch was zeigt. So dass dann irgendwann der Index selber langsamer wäre, als wenn man sich's noch mal selber raussucht und neu katalogisiert."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:54:32",
      "text": "Genau so, ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:54:33",
      "text": "Ah und das da hat noch keiner den goldenen Weg gefunden, wie man das auch ohne diese neue Anlegerei äh gut."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:54:40",
      "text": "Gibt es ein bisschen was, aber ähm was richtig in Produktion hat dann wohl tatsächlich nur die Firma mit den vielen Os glaube ich, ansonsten,\nIst es mir nicht so bekannt. Ich habe da jetzt lange nicht mehr geguckt, ob's der aktuelle Papers gibt, aber im Grunde musst du bei den meisten Indexen irgendwann neu anlegen, ja.\nDa gibt es verschiedene Strategien, wie du das herauszögern kannst, aber ähm es ist immer nur ein Herauszögern."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:55:04",
      "text": "Das heißt, selbst wenn man die Datenbank im Wesentlichen auch nur zum Abwerfen von von großen Mengen von Daten nutzt und noch gar nicht mal mehr drauf äh zugreift und diese Indizes sind schon eingerichtet, wird schon das Abwerfen langsam."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:55:18",
      "text": "Irgendwann wird schon das Abwerfen langsam, wenn sie, wenn die Datenbank on the Fly den Index aktualisiert, dann wird schon das Abwerfen langsam, ja. Wenn du sagst, ähm,\nMein Gott, tagsüber nehme ich die ganzen Daten auf, damit es schnell geht und dann lege ich nachts den Index neu an, dann hast du das gleiche Zeitproblem eben nur nachts, aber du hast es rausgeschoben, ne\nirgendwann ist schon so, wenn du."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:55:37",
      "text": "Und im Idealfall musst du's nicht doppelt und dreifach machen, sondern halt nur äh einmal, aber damit sind die Daten natürlich auch entsprechend später verfügbar, weil vorher kann man nicht drauf zugreifen. Hm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:55:49",
      "text": "Das ist das Kreuz mit den Indizes."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:55:52",
      "text": "Genau und das andere Kreuz ist natürlich, dass in dem Moment, wo man,\nDie also wenn man sich sozusagen seine relationale Datenbank eingerichtet hat, dann hat man ja im Prinzip auch schon die komplette Aussagefähigkeit,\nZumindest die Effizient durchführbare Aussagefähigkeit vorweggenommen. Dadurch, wie man die Tabellen strukturiert und auch, wie man diese Indices anlegt. Und wenn sich quasi die Fragestellung ändert."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:56:19",
      "text": "Dann hast du eine ganze Zeit lang ganz fürchterlich konstruierte SQL-Abfragen mit inner und outer joints und irgendwann äh\ndu nicht mehr durch, was sie bedeuten und dann setzt du dich mit allen Leuten an den Tisch und fängst an, die Tabellen neu zu designen, damit du diese riesigen zu verstehenden SQL Statements loswirst, ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:56:40",
      "text": "Und ist das nicht auch so so ein bisschen so der der Punkt, wo irgendwie big Data anfängt, dass man sagt,\nIst auch dann Big oder zumindest haben wir es jetzt sagen wir mal mit einer anderen Kategorie von von Datenermittlungen zu tun wo man eigentlich vorher auch noch nicht mal genau sagen kann was die Frage ist."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:57:04",
      "text": "Ja.\nDas ist mit Sicherheit auch wieder Big Data wenn es größer wird als der Raum, den du dir vorher ausgedacht hast oder wenn deine Fragen so kompliziert werden, dass die Antworten den Raum sprengen, bist du wieder im Big Data Bereich, klar."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:57:20",
      "text": "Also wie zum Beispiel die Frage nach dem Leben, dem Universum und dem ganzen Rest."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:57:25",
      "text": "Gut, die ist relativ klein, die Antwort, aber es gibt deutlich kompliziertere Antworten, würde ich sagen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:57:31",
      "text": "Gut, also worauf ich hinaus will ist,\nEs ist ja nicht so, dass jetzt rationale Datenbanken so ein Siegeszug angetreten hätten, dass sie wirklich für alles zum Einsatz kommen sollen. Es gibt einfach Felder,\nwo einfach Schluss ist, wo wo würdest du diesen diese Grenze ziehen? Also wo schließt sich quasi die Anwendung,\nEiner relationalen Datenbank von vornherein aus, wo es einfach dieses Konzept,\nerschöpft. Woran kann man das festmachen? Was schreit nach anderen äh systematiken."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:58:07",
      "text": "Vielleicht kann man da, ich weiß in der Zeit, als wir äh aus dieser aus dieser Internetzeit von mir, da hatten wir irgendwann einen jungen Programmierer, also als Internetprovider muss man,\nmusste man zu der Zeit auch schon auf einer wöchentlichen oder täglichen Basis den Kunden eine Statistik zur Verfügung stellen über die Abrufbarkeit ihrer Webseite beziehungsweise wie auf die Leute draufgeklickt haben, dass,\nDas war unheimlich wichtig, gerade für kleine Betriebe, für kleine Murkser, die einfach sich sehen wollten. Ich bin geklickt worden. Ich bin wichtig. Ich bin, genau, es funktioniert."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "0:58:41",
      "text": "Visitenkarte im Netz wird sie auch äh angeschaut."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "0:58:44",
      "text": "Wird angeschaut. Ähm also man muss ein Programm laufen lassen, was die Lockdaten äh.\nUnd wenn das immer mehr Webserver werden, immer mehr virtuelle Webserver, dann sind das auch immer mehr Lockdaten, die man kleinschneiden muss und protestieren muss. Und wir hatten damals einen jungen Programmierer, der hat gesagt,\nIch habe eine gute Idee. Wir nehmen, wie unsere Webserver war der Apaci. Wir nehmen die äh Apaci-Locks und wir packen die in eine Datenbank rein.\nDas ist super, dann kann ich äh aus anderen Rechnern verteilt mir jeweils für den jeweiligen virtuellen Server das Stückchen holen aus der Datenbank, was ich haben möchte und dann ähm,\nRechne ich darauf die Statistik aus. Und das haben wir, wir haben ihn machen lassen, glaube ich, als Spielwiese. Das war jetzt nicht das Produktionssystem, aber wir haben ihn,\nDas machen lassen, äh weil wir die Idee, warum nicht? Also ne, was Neues ausprobieren,\nUnd das hat nicht funktioniert, weil es äh weil die Daten, die wir hatten, schon zu Big für äh dieses kleine Datenbanksystem waren zu der Zeit, also der Apaci hat schneller Lockdaten produziert als Majes Duell, die wegspeichern konnte. Das hat äh\nauf einer vier Prozess oder Sun-Maschine eben, das ging einfach nicht. Ähm da,\nHätte man und zu der Zeit hätte man schon drauf kommen sollen, dass man vielleicht äh so was wie Mapre sich einfallen lassen könnte. Ich bin nicht drauf gekommen. Meine Kollegen auch nicht. Äh das haben das mussten doch noch ein paar andere machen, aber,\nÄhm ich denke, dass man diesen Fall immer wieder übertragen kann. Du wirst.\nProzesse, wenn du Prozesse hast, die schneller Daten erzeugen als deine Datenbank, sie wegspeichern kann, dann musst du dir was anderes einfallen lassen. Und dann kommst du auf neue Ideen.\nDie vielleicht nicht unbedingt eine relationale Datenbank. Oder du treibst den ganzen Ding, du schmeißt deinen ganzen deine ganze Gehirnkapazität darauf, diese relationale Datenbank immer schneller zu machen und zu parallelisieren.\nDas könnte ein anderer Ansatz sein."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:00:37",
      "text": "Jetzt Kurvenmaier gefühlt die ganze Zeit immer noch so ein bisschen Ende der neunziger Jahre äh herum. Ähm,\nund grade dann auch genau zum Ende des Jahrtausends hätte sozusagen kaum besser choreografiert werden können.\nTauchte halt auf einmal ein neuer Contender äh auf, nämlich Google, damals noch ein Universitätsprojekt,\nWas eben diese Problematik des Suchen im Webs einfach mal komplett neu schon erwähnt, alter,\nWar so bis dahin äh ja,\nDeshalb so bekannt, weil's halt eigentlich die einzige Suchmaschine, also es war nicht technisch wirklich die einzige Suchmaschine, aber es war mit Abstand das ambitionierteste äh Projekt, was zumindest für die damalige Zeit Verhältnismäßig,\ninteressante Ergebnisse liefert. Ich meine, man hatte jetzt auch den Vergleich nicht so äh richtig. Es gab auch verschiedene Suchmaschinen, Experimente äh in Deutschland. Weiß nicht, ob das schon in die Zeit ging, so mit\nMieter gehe, nee, das später, aber vorher gab's noch irgendwelche andere Sachen, wie auch immer. Auf jeden Fall.\nEs zeigte sich, dass dass man einen anderen Ansatz brauchte und was Google ja von vornherein so erfolgreich machte, ist, dass er erst mal das Konzept anders war, dass man nicht,\nähm nur,\nIrgendwelche Begriffe suchte und das rausspuckte, wo dieser Begriff am meisten drin vorkam. Das war so, glaube ich, so ein bisschen der Ansatz von Alter Vissa, sondern dass man quasi die Relevanz,\neines Suchergebnisses auch daran äh Bemessen hat, wie die Hypertext-Struktur innerhalb des Webs war, also dass etwas, auf das oft verlinkt wurde, dadurch eben sehr viel mehr Relevanz bekam."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:02:31",
      "text": "Genau, der legendäre Patrick, ne? Äh wie oft zeigt jemand anderes auf mich und wenn ich wenn mehr auf mich zeigen, bin ich relevanter als die entsprechende Seite, auf die weniger zeigen oder je öfter wir weniger oft gelingt wird.\nDas in Verbindung mit dem,\nOh Gott, wie hieß es? Es hieß nicht Google Fallsystem, es hieß glaube ich, das Erste. Ähm die Idee, ein,\nEin Pfeilsystem zu haben, was über Netzwerk und,\nRechnergrenzen geht, äh was sich repliziert, was reliable ist und was mit möglichst schmalen Ressourcen auskommt. Ich glaube, das war es war am Anfang, glaube ich, Big Table. Dann Google-Systemen ist das, was wir heute als kennen.\nAber die Idee mit wenigen, mit vielen kleinen Rechnern und einem verteilten Netzwerkfallsystem äh auch den Index zu zu zerteilen und ähm,\nDas zusammen mit dem Patrick hat den Erfolg ausgemacht, ne. Es war sofort deutlich besser als alles andere, die Ergebnisse und das hat eigentlich fast jeden überzeugt. Ich,\nWeiß noch, dass ein Kollege auf mich zukam und mir diese Seite gezeigt hat, mir ist das eine neue Suchmaschine und ich dachte mir, wow, das äh,\nspartanisch wow, aber ich glaube schon am Tag zwei äh dieser Übergabe äh war klar, dass das viel besser funktioniert als alles, was wir vorher hatten. Das war es war schon in der aller, allerersten Version so, ne?"
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:03:55",
      "text": "Ja, das stimmt. Google hat's einfach sofort halt einfach sofort überzeugt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:04:01",
      "text": "Eben auch algorithmisch, dass äh ich glaube, die erste Idee ist tatsächlich, glaube ich, dieser in der der Patrick gewesen. Ich weiß nicht, wann Big Table dazukam. Vielleicht als sie größer wurden. Da müsste man gucken. Du kannst es googeln."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:04:14",
      "text": "Ähm ja, das kam tatsächlich erst später. Also äh zweitausendvier ging das mit äh Big Table erst los.\nAber was hat denn jetzt Google anders gemacht oder wie hat Google das Problem der großen Daten ähm berücksichtigt, abgesehen jetzt von dem anderen Suchansatz?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:04:37",
      "text": "Ob sie das so früh schon gemacht haben, weiß ich auch nicht, aber ich äh würde jetzt mal, würde mich dazu versteigen zu behaupten, sie haben von Anfang an äh gemappt und redeused.\nSie haben die großen Datenmengen genommen und haben äh an einer Stelle etwas eingebaut, was den Daten, was sich in den Datenstrom einklingt und denen viele kleine Stückchen zerhackt und diese kleinen Stückchen an,\nViele Einzelrechner abgibt und sagt, kümmert um kümmert euch bitte um ein Teilproblem,\ndas ist in dem Fall hier den Index Wörter zu zählen in Tabellen zu packen,\nUnd wenn ihr fertig seid, dann gebt ihr eure vorberechneten Sachen zurück, eure vorberechneten Ergebnisse an einen Rechner, der sich darum kümmert, der das Ganze dann wieder reded, reduziert und die Ergebnisse zusammengefasst.\nIch bin ziemlich sicher, dass das von Anfang an ihr Ansatz war."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:05:28",
      "text": "Okay. Äh unabhängig jetzt von der zeitlichen Verortung. Also ich meine, das was du jetzt ansprichst, ist ja im Prinzip so das, was Google groß gemacht hat. Also selbst wenn sie jetzt das ein oder andere Konzept\nIn der allerersten Version nicht drin hatten und nur dadurch brilliert haben, dass hier überhaupt schon mal die Relevanz des des der Hypertextverlinkung berücksichtigt haben.\nWaren Sie ja auch selber sehr schnell vor die Situation gestellt, das eben wirklich mit sehr großen Daten, nämlich quasi allen zu tun zu haben, also allen des Netzes so, ne? Also jeder Webseite sozusagen, die sich in irgendeiner Form von außen,\nerreichen lässt, war für sie ja äh ein relevantes Datum, muss ja im Prinzip\nvorgehalten werden, um es durchsuchen zu können. Und es wirkt ja so ein bisschen absurd, dass man quasi, man hat so\nalle Computer und alle ihre Inhalte und dann gibt's noch mal einen Ort, da sind auch noch mal alle Inhalte, wie soll denn das gehen? Ich meine da da explodiert ja das äh Universum äh in gewisser Hinsicht, weil wie soll denn das, was sowieso schon mal da ist, noch mal reinpassen?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:06:24",
      "text": "Okay."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:06:26",
      "text": "War schon immer so ein bisschen das, wo ich so ein bisschen gezweifelt habe, ob ob die denn wirklich in der Lage also was denn jetzt eigentlich die die Tricks sind, um das äh zu machen. Aber du hast ja jetzt ja auch schon diesen Begriff genannt, Mapreduce.\nDem müssen wir jetzt irgendwie nochmal so ein bisschen äh bisschen erläutern. Wofür,\nIst dieser Map Reduce Ansatz jetzt sozusagen gemacht und was ist,\nWas ist so sein sein Revolutionäres, äh seine revolutionäre äh Kapazität?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:06:59",
      "text": "Du kannst dir vorstellen, wenn du das Problem in kleinere Stückchen zerlegst und auf viele verteilst, dass deine vielen kleinen Rechner, die sich um das Problem kümmern müssen, die müssen nicht mehr Big Iron sein."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:07:09",
      "text": "Ja gut, aber was ist das Problem erst mal?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:07:11",
      "text": "Das Problem ist eine eine riesige eine riesige Menge von Daten zu verarbeiten."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:07:15",
      "text": "Zu verarbeiten oder zu abzufragen oder überhaupt erstmal abzuspeichern."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:07:18",
      "text": "Ich glaube, hm zu verarbeiten, bleiben wir auf der Verarbeitungsebene, denn äh kann ich mich bei mit der kann ich mich anfreunden,\nÄhm nehmen wir an, du hast nehmen wir doch ein konkretes Beispiel. Du hast die gesamte Wikipedia und du willst sagen, du willst äh das Vorkommen des Wortes Katze zählen.\nIn der gesamten Wikipedia. Das sind ein paar oh Gott sind das Teerabyte an Daten? Das sind auf jeden Fall einige hundert."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:07:46",
      "text": "Viel ist big."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:07:47",
      "text": "Hundert Gigabyte ein Spik. Ähm,\nUnd jetzt kannst du durch diese große Datei durchgehen und kannst an jeder Stelle, wo das Wort Katze vorkommt, äh equementierst du deinen Zähler und hast am Ende ermittelt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:08:00",
      "text": "Meine Wikipedia ist ja jetzt tatsächlich liegt ja in SQL."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:08:05",
      "text": "Verdammt.\nNehmen wir an. Stimmt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:08:12",
      "text": "Also man könnte sicherlich auch anders ablegen, aber äh."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:08:15",
      "text": "Also du bist auf eine irgendwie geartete Art äh äh an an Askidamp der Wikipedia geraten äh und willst\nUnd willst du ihn jetzt prozessieren äh und dein Rechner ist viel zu klein, aber du hast äh in dem Büro, in dem du sitzt, sind noch 15 Arbeitsplatzrechner,\nUnd mit denen willst du das jetzt machen, das Problem lösen und die Antwort soll auch nicht nächste Woche Mittwoch da sein, sondern vielleicht morgen Abend.\nDann würdest du auf die Idee kommen über Map und Rede nachzudenken, der Rechner, an dem du sitzt, übernimmt das Zerschneiden, alle,\nzwei Megabyte wird ein Päckchen geschnitten sinnvoll. Aus diesem Askidamp und wird an diese vor sich hin eilenden Arbeitsplatzrechner äh geschickt automatisch."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:09:00",
      "text": "Aber es geht euch jetzt davon aus, dass die Wikipedia in einem kontinuierlichen Datenstrom vorliegt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:09:04",
      "text": "Genau, das wäre ja ein Aski-Dump, oder? Also wenn du durch einen Aski-Dump durchgehst, fängst du am Anfang an und dann gehst du Wort für Wort durch, bis du am Ende bist und."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:09:12",
      "text": "Und und und die Information, wo jetzt eine Seite anfängt und so weiter, die ignorieren wir jetzt einfach oder äh."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:09:17",
      "text": "Ich will nur wissen, wie viel, wie oft das Wort Katze vorkommt. Das ist jetzt ein bisschen arg konstruiert, aber so ist es halt. Also in dem Problem, das könnte man mit Map Reduce."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:09:28",
      "text": "Okay und was mache ich dann? Was mappe ich und was reduziere ich?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:09:32",
      "text": "Hm du mapst die einzelnen Pakete äh an die an die kleinen Rechner, die sich um das äh,\nzählen in den Einzel äh Paketen kümmern und das Reduzieren ist das Zurücknehmen,\nDer jedes jeder Einzelrechner hat jetzt für sein Stückchen Textdatei das Wort Katze gezählt. Der erste Rechner hat 15 gezählt, der nächste Null, der übernächste und das reduzieren ist jetzt alle schreien. Ich bin fertig mit meinem Paket.\nUnd geben ihre Ergebnisse an den Regierungsrechner zurück und der übernimmt jetzt bei 15 Rechnern den Job fünfzehn,\nZahlen zusammenzuzählen und am Ende weißt du, wie oft das Wort Katze in deinem Dampf vorkam."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:10:14",
      "text": "Okay, also klingt für mich jetzt erstmal nur für wie im Prinzip eine einfache ähm Verteilungsstrategie."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:10:21",
      "text": "Ja, im Grunde ist es eine Verteilungs- und Berechnungsstrategie. Es kommt mit ein bisschen mehr daher. Du kannst, musst dich um das ähm,\nUnd das Verteilen der Pakete und das Zusammenführen, das übernimmt das Protokoll für dich. Da musst du dich nicht kümmern. Das musst du alles nicht selber schreiben, sondern du musst nur im Grunde diesen Kernalgorithmus formulieren. Du musst einmal sagen, wie wird das Paket zerschnitten und was sollen die Kleinens rechnen?\nUnd den Rest übernimmt das Framework für dich. Da gibt's unterschiedliche Frameworks. Nicht alle sind gleich, aber,\nbei den meisten ist wenigstens das implementiert, was ich gerade beschrieben habe und das ist auch fast immer ähm in jedem in jeder Einführung das blöde Beispiel mit dem Wörterzählen drin, was einem nicht so richtig klar wird, was das soll, aber äh tatsächlich."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:11:05",
      "text": "Jetzt auch hier."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:11:06",
      "text": "Mhm. Jetzt auch hier. Mir ist nichts besseres eingefallen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:11:09",
      "text": "Oh je. Wir wollten das ja vorhaben, um ein bisschen über dieses Lernen über die lernenden Maschinen unterhalten, weil äh,\nDer ganze äh Klimbim, den wir jetzt quasi angeschnitten haben, das waren ja jetzt primär vor allem erstmal so ein bisschen Effizienz verbessernde,\nMaßnahmen, die überhaupt erst mal technisch ähm die Datenmengen greifbar gemacht haben, die sozusagen ein effizientes Zerschneiden ermöglichen,\nUm einfach dem eigentlichen Traffic und der Speicherung Herr zu werden und das sozusagen auch gut skalieren zu können, mit wachsende Systeme zu schaffen.\nDie eigentliche Frage ist ja immer wie kriegt man jetzt aus diesen ganzen Datenwald auch wirklich Antworten,\ndie möglichst intelligent sind. Ich meine, dieses Zählen von Katzen okay, das back ja in gewisser Hinsicht äh ähm einen vielleicht auch mal interessieren, aber eigentlich will man ja wissen, was denken sich diese Katzen eigentlich die ganze Zeit?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:12:11",
      "text": "Das will man eigentlich wissen genau. Das sind Sachen die."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:12:15",
      "text": "Ist das eine Verschwörung, werden wir von Katzen kontrolliert? All das, ne? Leo Cats können ja auch nur eine ein Ablenkungsmanöver gewesen sein."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:12:26",
      "text": "Vermutlich sind Sie das gewesen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:12:27",
      "text": "Ja ja sie ist klein Aliens. Die versuchen unsere Hirne zu steuern."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:12:34",
      "text": "Also tatsächlich ist es glaube ich relativ schwierig rauszukriegen, was Katzen so denken aus Webinhalten stelle ich's mir tatsächlich äußerst schwierig zu beantworten vor."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:12:44",
      "text": "Wie auch immer, auf jeden Fall hatten wir ja äh also auch da kann ich jetzt mal so ein bisschen in meine Historie gehen. Ich meine mich zu erinnern, dass irgendwann mal so in den,\nWar das schon in den achtziger Jahren,\nneunziger Jahren. Irgendwann fingen halt alle an, von neuronalen Netzwerken äh zu quatschen. Das war irgendwie schon relativ früh, mal so ein Thema. Ähm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:13:05",
      "text": "Ja die waren irgendwo zusammen mit den Frackalen, ne? Also das war so ein ein der Domäne. Ich habe zu der Zeit habe ich ja Mathe studiert und man hat dann entweder hat man neuronale Netze gemacht oder man hat frackale gemacht. Eins von beiden, das war\nEs gab praktisch keine andere."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:13:20",
      "text": "Das eine hatte auch noch richtig bunte Bilder, ja."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:13:23",
      "text": "Genau.\nUnd das andere hat nicht so richtig gut funktioniert. Jedenfalls immer nur bis zu einer bestimmten Teilmenge und danach ist es äh grandios in die Hose gegangen. Ähm,\nIn den achtziger Jahren kam's auch überhaupt erst auf, dass man mit Rechnern,\nPlötzlich so viel Rechenpower unterm Schreibtisch auf dem Schreibtisch hatte, dass man bestimmte Sachen rechnen konnte und wollte und hat,\nBestimmte komplexe vorher nie angefasst, weil es nicht ging.\nKlar, da gab's paar Großrechner, die haben äh im universitären Bereich oder im militärischen Bereich oder im meteorologischen Bereich, das haben wir alles schon durch, da hat man bestimmte Probleme gelöst, aber da war eben auch Kapazität da vorher,\nAuf dem Schreibtisch nie. Plötzlich war überall genug Rechenkapazität CPU mäßig da, dass man bestimmte Sachen,\nbeantworten konnte. Also hat man angefangen, neue Fragen zu stellen. Zum Beispiel kann ein Computer vielleicht eine Unterschrift lesen. Man konnte,\nDie ersten Scanner waren da. Vielleicht könnte man eine Unterschrift digitalisieren. Auch eine Sache, die heute selbstverständlich ist. Zu der Zeit nicht. Da hat man sich das eher gefragt, wie das gehen könnte.\nUnd dann kommen wir zum ersten großen Run des maschinellen Lernens auf die Computer. Man hat bestimmte Verfahren angewendet, ausprobiert,\näh die evaluiert, wie gut funktionieren sie und unter anderem kamen damals eben schon sehr früh die neuronalen Netze. Die hat man benutzt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:14:51",
      "text": "Von welchem damals reden wir jetzt gerade?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:14:52",
      "text": "So aus den aus den Achtzigern, ne, so fünfundachtzig, 86 in dem Dreh. Ähm,\nMan hat die damals die kamen die neuronalen also die neuronalen Netze sind nur ein Aspekt des,\naus der ganzen aus dem großen Werkzeugkorb des maschinellen Lernens. Die sind heute sehr, sehr erfolgreich damals 86 waren sie das gar nicht,\nDa haben sie es gibt ein klassisches Beispiel ähm es gibt den Amnest Datensatz, das sind äh gescannte Zahlen, die,\nIch glaube Amerikaner mit Kugelschreiber auf Wahl oder Steuerpapiere geschrieben haben und die Amerikaner haben das gescannt und diesen Datensatz den gibt es schon ewig,\nDM\nÄhm,\nDas ist sozusagen das Hello World, das Maschinenlair und des visuellen Maschinen Learnings, äh diesen zu prozessieren.\nDie Aufgabe ist, äh diese gescannten Zahlen zu erkennen.\nDa sind Grauwerte drin, 8undzwanzig mal 28 Grauwerte äh und hinten steht ein Label, das ist eine Null, eine Eins, also das sind null bis neun und davon gibt es glaube ich 60.000 in diesem MS-Datensatz.\nUnd du musst jetzt ein Verfahren trainieren, du musst dem etwas konstruieren und dann ein Verfahren trainieren auf diesem Datensatz.\nDu teilst den irgendwie in 90 Prozent zum Lernen und zehn Prozent zum Testen und mit 90 Prozent lernst du dein Verfahren an und mit 1 Prozent testest du dann, wie gut es ist.\nUnd das hat man damals schon gemacht mit diesen neuronalen Netzen, zwar nicht mit 60tausend, sondern ich glaube nur mit 1000 und hat festgestellt, das funktioniert hervorragend,\nBis zum bis zur ich weiß es nicht mehr, 300 Zahl und dann brechen die Ergebnisse bricht der Ergebnisraum der vorher ganz okay war komplett zusammen und das Netzwerk antwortet nur noch,\nkompletten Bullshit. Das ist was, was man."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:16:40",
      "text": "Okay, ähm jetzt hören wir erst mal noch mal ein bisschen ins Detail gehen. Neuronales Netzwerk. Was heißt das genau?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:16:49",
      "text": "Ein streng genommen ist es ein artifizielles, neuronales Netzwerk, weil die neuronalen Netzwerke, die äh haben wir im Kopf und äh Tiere auch. Also jede Art von Nervenverknüpfung ist äh,\nIn den allermeisten Fällen ein neuronales Netzwerk. Also."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:17:05",
      "text": "Aber also was hat man sozusagen im Hirn gesehen, von dem man der Meinung war, dass es das dass es sinnvoll ist, dass in Software nachzubilden? Also was genau was wir von unseren Neuronen kennen, wird darin abgebildet."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:17:20",
      "text": "Ja, die,\nDas Ganze kommt äh eher aus der aus den Kognitionswissenschaften und die haben schon sehr früh angefangen, Affengehirne auseinanderzuschneiden, sehr unappetitlich, aber wenn man was übers Gehirn lernen will, dann äh kommt man nicht umhin, äh so was zu tun.\nUnd schon in den 60ern hat man festgestellt, dass der visuelle Cortex, also die Verbindung der,\nAm rückwärtigen Teil des Sehnervs geht ein dickes Nervenbündel an ein bestimmtes Areal im Hirn, das ist am Hinterkopf verortet. Das ist der visuelle Cortex.\nUnd wenn man praktisch ungeborene Affen nimmt und schneidet dieses Gebiet auf, stellt man fest, dass dieser Zellenteil, der adressiert wird von den,\nVon den Nerven bündeln, dass der sehr ungeordnet äh und chaotisch ist. Da hat sich da passiert nichts. Und wenn man das jetzt mit jungen Affen immer wieder macht im Zeitraum von ein paar Wochen stellt man fest, dieser."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:18:13",
      "text": "Was mit ihm wieder macht."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:18:14",
      "text": "Äh sie sie tötet und den den Kopf aufschneidet und."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:18:18",
      "text": "Immer wieder tötet."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:18:19",
      "text": "Sie immer wieder Töter sind."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:18:21",
      "text": "Also immer ältere äh mhm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:18:22",
      "text": "Immer ältere Affen, genau in unter in unterschiedlich, also oh Gott."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:18:24",
      "text": "Entschuldigung."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:18:28",
      "text": "Der arme Affe.\nAlso immer immer äh ältere Exemplare der selben Affenart äh nimmt und jedes Mal den Kopf äh aufschneidet, um diesen Bereich zu untersuchen, stellt man fest, dass,\nDiese Nerven, dieses Areal äh sich auf eine komplexe Art ordnet,\nSehr sehr stark schon aufm unterm Mikroskop zu sehen in den sechziger Jahren. Äh."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:18:53",
      "text": "Also Verbindung zwischen Nervenzellen neu entstehen, die es so vorher nicht gab."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:18:57",
      "text": "Genau. Vorher war das alles so ein bisschen chaotisch und plötzlich sieht man, da hat sich gewaltig was getan. Das."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:19:03",
      "text": "Das heißt, man hat im Prinzip festgestellt, dass das nicht so eine vorgegebene Struktur ist, die ein bestimmtes Prozess eben macht, sondern dass quasi Teil der Entwicklung ist das neue Verdrahten."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:19:14",
      "text": "Genau, während der Affe sieht und ähm bei Affen weiß man das genauso wie bei uns. Man muss Babys das Sehen nicht beibringen. Das passiert voll automatisch. Auch das Hören nicht.\nDass es irgendein Teil, den unser Gehirn mit einer einfachen Vorcodierung in diesem neuron, in dieser neuronalen Struktur ist ein sind ein paar einfache Befehle drin, aber wirklich äh ein, zwei und sehr, sehr einfach.\nUnd mit diesen Dingen, mit diesem,\nMit diesem neuronalen Netzwerk und diesem dieser einfachen Anweisung optimiere die Energie von einer Ebene auf die andere ähm ist das in der Lage,\ndas sehen zu lernen,\nDas hat man in den 60er Jahren schon rausgefunden. Hm, das fand man gut äh und in den 80ern ist man und man hat auch schon in den 60ern und in den Jahren danach angefangen, das mathematisch zu modellieren.\nSo der erste Neuronenmodelle mathematischer Art zu schreiben äh und in den 80ern konnte man das,\nDas erste Mal codieren, weil die Rechner Leistungsfähig genug waren und hat dann genau dieses Problem visueller Cortex, sehr sehr vereinfacht, achtundzwanzig mal 28 Pixel nur in der Lage Grauwerte zu sehen,\nLöst das Problem. Lerne das Sehen. Lerne eine Struktur, lerne zehn verschiedene Strukturen voneinander zu unterscheiden."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:20:29",
      "text": "Also Form Dreiecke, Kreise."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:20:32",
      "text": "Genau aus was so eine Zahl zusammengesetzt ist, nur eine handgeschrieben, ne\nDas Dumme ist, das Handgeschriebene, dass wenn Menschen Zahlen schreiben, dann machen die das nicht immer auf dieselbe Art und Weise, weil die nämlich keine Schreibmaschine sind, sondern das sieht oft sehr krakelig aus und der Algorithmus muss mit dieser Varianz umgehen können. Also der muss ein,\neine eckige zwei von der ganz runden zweiäh unterscheiden beziehungsweise in denselben Topf werfen kann. Das macht das Problem sehr komplex. Ähm,\nMan könnte es vielleicht auch wie eine äh sehr komplexe Art der Datenreduktion bezeichnen, weil wenn ich mir das Grundproblem angucke, ich habe achtundzwanzig Mal achtundzwanzig,\nGrauwerte, das sind ähm,\nMathematiker, das sind einige hundert Werte ähm mal 256 Grauwerte, die zugelassen sind."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:21:26",
      "text": "Siebenhundertvierundachtzig."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:21:27",
      "text": "Siebenhundertvierundachtzig. Hm also ich habe eine Reduktion von einem 784 dimensionalen Problem auf ein zehndimensionales Problem,\nne? Unten sind die Grauwerte und oben mache ich zehn Lampen an. Die erste Lampe heißt, ich habe eine Null. Die letzte Lampe heißt ich habe eine Neun.\nDas äh ist so eine Art,\ndieser visuelle Cord, dieser künstliche, visuelle Cortex oder auch der visuelle Cortex, den wir im Kopf haben, macht eine Reduktion jeder einzelne nervt des Auges. Das sind auch eine ganze Menge. Jede,\nPunkt der Dakodiert rauskommt. Den willst du nicht direkt ins Großhirn leiten, sondern du willst ihn irgendwie vorprozessieren. Du wirst sagen, okay, links ist eine rote Fläche,\nHier unten ist ein Dreieck, das da ist wahrscheinlich ein Baum und das große Hirn muss sich nur noch ähm Baum ist schon eine Bewertung, sondern der macht\nSo eine geometrische Vorsortierung und das Großhirn übernimmt dann die,\nDiese Daten, die an anders geliefert werden und übernimmt die Bewertung und sagt, okay, du stehst in einem Wald, weil hier dieses Grüne da oben, ne, dass diese ganzen Flächen, das sind Bäume und das da unten,\nJa und dieser Teil der ähm eben aus der Konditionswissenschaft kam früh mathematisch schon beschrieben wurde. In den 80ern erfolgreich äh erfolglos."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:22:47",
      "text": "Also erfolglos war er, weil man zwar gesehen hat,\nOkay, der Ansatz scheint an sich schon mal weit zu tragen. Man hat quasi äh gute Ergebnisse, die man so mit einem anderen Ansatz auch nicht so ohne Weiteres äh hätte erzielen können, aber er versagt irgendwann."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:23:04",
      "text": "Genau. Also man wusste oder man weiß ja sehr genau ähm,\nUnser System im Kopf nicht so funktioniert. Wenn wir was völlig Neuartiges sehen, dann sind wir zwar eine Weile verwirrt, aber irgendwann äh ordnet sich das, wenn wir das einfach nur angucken. Und wenn nicht, müssen wir's ein paar Mal, wenn man sich,\nneue eine neue Art von Kunst anguckt oder man kennt das bei so Wechxierbildern, bei so optischen Täuschungen. Wenn du dir das drei, vier Mal angeguckt hast, dann hast du das gelernt. Du musst dann beim nächsten Mal weißt du genau, zack, das das, das ist der, das ist Trick Nummer dreizehn, den habe ich jetzt,\nUnd genau diese Eigenschaft hatte dieses artifizielle neuronale Netzwerk aus den 80er Jahren nicht. Das hat eben Beifall,\ntausenddreiundzwanzig oder beim bei irgendeinem neu konstruierten Fall hat es,\nNicht nur gar kein Ergebnis mehr geliefert, sondern es hat ab daher nur noch falsche Ergebnisse geliefert. Und da wusste man schon, hier stimmt was nicht. Irgendwas ist hier in der ganzen Struktur falsch."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:24:01",
      "text": "Man wusste nicht was."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:24:02",
      "text": "Man wusste nicht was. Man hat sich das gefragt. Das ist sehr lange erforscht worden immer wieder äh aber,\nNun ist Maschine Learning auch immer was sehr praktisches, weil meistens ist es praxisgetrieben. Du hast irgendwas, was du mit dem Computer berechnen willst. Jemand hat dir eine Frage gestellt und äh,\nDu sollst nun was konstruieren und damit ist das Verfahren einfach mal in der Schublade gelandet, weil es gab eine andere Art von ähm,\nvon maschinellen Lernverfahren, die sogenannten Körnermaschinen. Die kennt man als Support-Vektormaschinen. Die sind unglaublich erfolgreich geworden in der Zeit.\nUnabhängig von dem anderen Kram, was sowieso die ganze Zeit immer gut funktioniert hat, wie bei und und was es da alles gibt.\nAber diese Körnermaschinen äh traten eben in den Achtzigern in den ins Rampenlicht und haben,\npraktisch 20 Jahre lang das maschinelle Lernen dominiert. Da gibt es unglaublich viele Untersuchungen, Papiere. Das ist auch so ziemlich jedes Sortierproblem äh,\nDer Welt optimiert worden und ähm ja eigentlich,\nwar die Welt schön und einfacher. Man hat gesagt okay, man hat so Port-Vektormaschinen, damit kann man Texte äh klassifizieren und man kann Zahlen klassifizieren, man kann alles wunderbar klassifizieren."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:25:22",
      "text": "Hat das performt auf dieser Handschriftsdatenbank, auf dieser Datenbank."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:25:27",
      "text": "Sehr gut. Ich beweise nicht die Zahlen. Ähm vielleicht hast du eine Tabelle vorliegen, aber es war,\nÄh ich denke, da ist man weit über 90 Prozent gewesen auf dem Ende des Datensatz,\nAber der geht auch da gibt's so es gibt da in der so eigene die jedes Jahr ausgefochten werden äh da trifft man sich und dann,\nVermittelt man dem anderen,\nDen Kniff, den man grad an seinem Algorithmus getweekt hat, um 0,3 Prozent besser zu werden oder 3 Prozent, dann ist es schon viel und so iteriert man sich da durch die Prozenträume von Jahr zu Jahr weiter, aber Amnest ist so alt, ähm da dass es,\näh dass da fast alle Verfahren an ihrem Rand sind. Also das wird nicht mehr besser.\nUnd dann ist ja lange Zeit nicht viel passiert und 2tausend7 ist ein Papier rausgekommen von einem,\nähm von einem britischen Professor, der in Toronto äh eine Professur innehat. Der Mann heißt Jeffrey Hinton und der hat ein Papier rausgebracht, was ähm,\nWenn man in diesen maschinellen Lernen erfolgreich äh oder sagen wir inkrementelle Schritte,\ngewöhnt ist, die jedes Jahr ein paar Prozent besser werden. Er ist auf äh auf einen Schlag äh im zweistelligen Prozentbereich besser geworden und äh das hat ihm erstmal so niemand so richtig geglaubt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:26:58",
      "text": "Also in welcher Disziplin jetzt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:26:59",
      "text": "Ich glaube."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:27:01",
      "text": "Bei den neuronalen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:27:02",
      "text": "Bei den neuronalen Netzen. Also man hat meistens irgendein Problem, den wir jetzt Amnest. Da gibt's noch ganz andere Datensätze. Es gibt den Iris-Datensatz, der auch sehr ähm,\nbekannt ist oder die twenty News Groups. Da geht es darum, Texte zu klassifizieren. Das sind alles verschiedene Teile. Ähm wo man sagen kann, ich habe mein Verfahren hier drauf losgelassen und ich bin so und so viel Prozent besser als alles vorher.\nOder ich habe das gibt es Scorings, die heißen Precision, Recall und F 1scall, das hat man vielleicht schon mal gehört. Das eine sind so ähm,\nWie wie genau man ist, äh man kann beliebig genau werden mit fast jedem Verfahren, aber man wird dann so genau, dass man auch noch äh Sachen hm dazu nimmt, die gar nicht richtig sind. Ähm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:27:45",
      "text": "Also Forrest Positives sozusagen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:27:47",
      "text": "Also diese dieses äh Scoring, das gibt man meistens absolut an. Man hat da so irgendwie sein neues Verfahren, hat bekannte Datensätze, die jeder kennt und gibt dann an so sieht man, so sieht man Record und so sieht man F 1 Score aus und dann weiß man, wo man ist, im Ranking.\nUnd das äh hat äh Herr Hinten mit seinem Verfahren, das ist ein relativ langweiliger Papertitel. Äh,\nIch weiß nämlich nicht auswendig. Ähm guck mal."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:28:12",
      "text": "Guck mal."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:28:15",
      "text": "Man viel schöner googeln kann, äh ist ähm so ich glaube 2tausend7 äh Google und hinten, dann kommt man auf YouTube auf ein Video, äh haben ihm,\nDie Google-Leute haben ihn eingeladen, weil da gab's wohl zwei, drei Leute, die gesagt haben, okay, was der Mann da sagt, gibt Sinn. Äh und zwar absoluten Sinn. Und dann gibt es eine legendäre Vorlesung. Die haben die auf Video aufgezeichnet.\nUnd das kann man schon so ein bisschen als den Startpunkt von dieser neuronalen Revolution bezeichnen. Damit ging's so richtig los.\nDa erzählt er äh relativ langweilig, was er so gemacht hat und äh,\nwarum neuronale Netze damals nicht funktioniert haben und wo man sich verrechnet hat und wo man ähm Gradienten abgeschnitten hat und was man machen muss, wenn man Schichten trainiert. All solche Geschichten werden da erklärt.\nDas Ergebnis ist, das Verfahren ist so viel besser als alle anderen Verfahren des maschinellen Lernens und so viel einfacher in,\nIm in der Fite-Ex-Aktion, dass man das,\nWas für die meisten, für die meisten Bereiche des maschinellen Lernens äh heute neuronale Netze verwandt werden."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:29:23",
      "text": "Okay äh ich versuche das mal ein bisschen zusammenzufassen. Also man hat,\nIn den achtziger Jahren hat man so ein bisschen die Ahnung gehabt, dass das irgendwie äh der Ansatz sich am an dem, was man zum dem Zeitpunkt vom Hirn äh,\nVerstand, insbesondere dem Bereich des äh visuellen äh Cortex, dass das sozusagen ein Ansatz ist, wie man auch selber das Lernen\nIn der Maschine replizieren kann und dabei möglichst effizient zu werden.\nUnter anderem anhand dieser Beispiel mit äh visueller Erkennung, Handschrift äh et cetera. Hat aber dann festgestellt, dass man irgendwie mit diesen Ansätzen, die man da verfolgt hat, zwar weit, aber eben,\nNicht nur nicht nicht beliebig weit kam und dass es dann vor allem auch dass man dann irgendwann so ein bisschen gegen die Wand äh gefahren ist und dann ging's einfach überhaupt nicht mehr weiter.\nJa und dann äh,\nHat man das jetzt so 20 Jahre so ein bisschen äh beiseite gelegt. Dann sagt er, okay äh war ein schöner Traum. Ähm jetzt gucken wir uns mal hier die paar anderen kleinen Sachen an und guck mal,\nFunktioniert ja auch irgendwie ganz gut, kann man aber alles schön zusammenschieben, ist einfach zu implementieren, kommen wir auch so weiter.\nUnd dann wie aus dem Nichts stieg äh hinten sozusagen äh auf. Und jetzt habe ich hier zwei Paper äh für dich im,\nAngebote, kannst du jetzt mal sagen, welche das jetzt ist, hier to recourse, shapes first learn to generte images hatte 2tausend7 gemacht und."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:30:50",
      "text": "Das zweite Learning Multiplayers of Representation.\nDas ist der Beginn von dem, also weil es sind multiple Layers, ähm die neuronalen Netze der achtziger sind man spricht da so von verschiedenen Schichten der Eingangsschicht, der Ausgangsschicht und der verdeckten Schicht von Neuronen,\nUnd äh mit was wir heute arbeiten, das sind sogenannte Deep-Learning Models, das heißt einfach nur, dass die verdeckte eine Schicht jetzt mehrere geworden sind im besten Fall sogar mehrere hundert oder mehrere tausend.\nJede Ebene lernt von der anderen. Die unterste Ebene kümmert sich nur darum, um jetzt mal bei dem visuellen Cortex zu bleiben Pixel zu erkennen, ne, also bunte Pixel, da ist was Rotes, hier ist was Schwarzes, äh\nBeziehungsweise Nervenenden,\nÄhm und die Ebene darüber verbindet Pixel, die zusammenhängen zum Beispiel. Zwei Schwarze, die direkt zusammenhängen, ergeben vielleicht einen Strich.\nDann hat man eine Ebene, die so ja schon so zwei, drei Pixel kombiniert hat und die Ebene darauf, die Neuronebene, die trainiert,\nSchon nicht mehr auf den Pixel, sondern die wird trainiert auf den Strichen, auf den zusammengesetzten Strichen und setzt daraus wieder was Komplexeres zusammen. Und jetzt wandert man so Schicht für Schicht immer höher,\nNeuronenschicht für Neuronenschicht und jede lernt kompliziertere Dinge und auf der oberen, obersten Neuronenschicht erkenne ich dann im Gesicht und sage, ah guck mal, Tim oder,\nEine Marte Flasche oder hm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:32:12",
      "text": "Ja.\nOkay und das war jetzt wirklich so äh neu. Ich meine im Prinzip würde ich ja fast meinen dass gut jetzt kann ich natürlich klug daher reden, aber ähm,\nfunktioniert Abstraktion nicht so, also ist,\nirgendwie klar, dass man vom vom Kleinsten ins größte äh in in in mehreren Schritten gehen muss. Also was hat man denn damals so weggelassen? Das das man diesen Weg so halt nicht gehen können."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:32:44",
      "text": "Hm also einmal hat man die Rechenkapazität nicht gehabt und dann hat man sticht,\nschlicht, weil es ist eine Folge der Rechenkapazität gewesen und hat die Berechnung an einer bestimmten Stelle abgekürzt und hat gesagt, ab hier brauchen wir die Nachkommenstellen nicht mehr. Die tun wir mal weg, weil das funktioniert ja ganz wunderbar und äh,\nDas war ein Fehler."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:33:06",
      "text": "Das hat sozusagen nie einer mal mit Nachkommastellen probiert."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:33:10",
      "text": "Ich glaube das weiß ich nicht, ob das mal jemand probiert hat, aber wenn, hat das nicht auf komplexere Netze angewandt äh,\nAlso tatsächlich funktioniert Intuition und Abstraktion so und äh nicht in die Abstraktion funktioniert so. Man wird immer komplexer. Ähm das Problem ist, wenn du Maschinen so was beibringen willst, musst du diese,\nDiesen,\nDiese Zunahme an Komplexität, die musst du ja mathematisch modellieren. Und da äh haben wir ein Problem gehabt. Das hat einfach ein bisschen gedauert. Das geht jetzt. Ähm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:33:41",
      "text": "Was muss da mit modelliert werden."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:33:44",
      "text": "Das Lernverfahren für neuronale Netze, das heißt und das Lernverfahren für,\nmehrere Netzwerke nähere aufeinander liegenden Netzwerke oder zeitlich miteinander äh verknüpfte Netzwerke rekursive Netzwerke, das heißt dann back Propagation und äh das ist,\nNichts mathematisch Kompliziertes, aber wenn du das nicht ordentlich machst, dann äh verrechnest du dich oder das Verfahren funktioniert gar nicht und,\nDas hat einfach eine Weile gebraucht, bis das soweit war. Also die die Basis für Back Propagation, die findet sich auch schon in den achtziger Jahren, aber,\nDass man das so zusammensetzen kann, dass es funktioniert. Das ist alles äh eine relativ neue Art. Auch die Art und Weise, wie man die Einzelschichten trainieren kann, äh ohne das ganze Netzwerk an sich zusammenbrechen zu lassen, das ist auch was.\nWas wir erst in diesem Jahrtausend hinbekommen haben."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:34:37",
      "text": "Und das haben wir ja alles durch äh diesem Herrn Hinden zu verdanken sozusagen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:34:42",
      "text": "Ich würde sagen, man kann den schon als äh Stein des Anstoßes bezeichnen. Der also sind natürlich sofort viele andere Leute mit aufgesprungen, aber die das Hauptpaper, der Anstoß kommt schon von ihm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:34:54",
      "text": "Und was hat er das so für für Auswirkungen? Also,\nWaren da alle überrascht oder war äh dachten alle nur so, ja schön, dass mal einer äh das zu Ende äh gerechnet hat, also woran konnte er denn sozusagen festmachen, dass dass das jetzt äh,\nDass ein, ja, dass dass er ein ein ein konzeptioneller Durchbruch ähm stattgefunden hat."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:35:18",
      "text": "Hm, also wie vorhin schon beschrieben, kann man bei all diesen Dingen eben äh das kann man in Zahlen angeben, wie viel besser man wird. Und wenn man ähm man bei so maschinellen Lernverfahren ist es immer so, man beurteilt verschiedene Dinge. Man beurteilt wie schnell so ein Verfahren lernt.\nDass es bei neuronalen Netzwerken gar nicht so richtig gut. Die lernen sehr langsam, das will man eigentlich nicht, weil,\nAlso klar, die Antwort von einem gelernten Dingen, die soll die kommt immer schnell oder die sollte schnell kommen, sonst wird das Verfahren meistens aussortiert. Das ist bei neuronalen Netzwerken so, die Antwort ist rasend schnell da, aber das Training, das braucht eine Weile, das ist kann man sich so ein bisschen,\nÄhm wie so eine Erziehungsgeschichte vorstellen. Nur man muss eben so oft die Sache,\nSehen, lernen, bis man sie verstanden hat. Das ist jetzt ein bisschen sehr flapsig formuliert, akademisch kriege ich dafür keine Krone. Hm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:36:11",
      "text": "Du bist ja hier auch äh im Podcast, da kannst du kannst du ja ein bisschen salopper zuwerken."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:36:15",
      "text": "Oh okay.\nUnd das andere ist, dass man man kriegt mit den vorhin zitierten Support-Vektormaschinen die meisten Probleme ganz, ganz wunderbar gelöst.\nAuch gar nicht viel besser. Die neuronalen Netze sind auch gar nicht so viel besser in absoluten Zahlen, aber du musst ähm,\nDas nennen wir gerne Feature Engineering. Du musst aus deinen Daten die relevanten Teile raussuchen, die wichtig für die Beurteilung oder für die Beantwortung deiner Frage sind. Wenn du,\nÄhm wir haben vorhin,\nBei den Wörtern, wenn wir jetzt bei den visuellen Geschichten bleiben, ist es äh zum Zahlenerkennen vielleicht absolut ausreichend nur die schwarz-weißen Werte anzugucken und du brauchst die Farbigen gar nicht. Wenn du dir mit Kugelschreiber ausgefüllte Lottoscheine anguckst,\nfür ein anderes Problem ist Farbe vielleicht total wichtig, wenn du irgendwas anderes beurteilen willst,\nUnd ähm wenn du das mit klassischen Maschinenlearning-Verfahren beantworten willst, brauchst du Leute, die in der Domäne zu Hause sind, deiner Problemdomäne und die müssen,\nSagen, guck auf diesen Teil des Wertes, der ist wichtig. Ähm und dann kann die Support-Vektormaschine oder irgendein anderes Verfahren.\nMeistens äh sehr gut eine Antwort geben, wenn das Training ordentlich war,\nDie neuronalen Netzwerke haben den Vorteil, dass sie diese Features meistens selber lernen, nämlich die unterste Ebene, unteren unteren Ebene der neuronalen Netze lernen die Struktur.\nUnd die oberen abstrakteren Layer der neuronalen Netzwerke beantworten deine Frage,\nUnd das ist toll, weil du baust plötzlich äh niemanden mehr, der deine Features die ganze Zeit scharf anguckt und überlegt. Da steckt viel Brain Power drin, das zu machen, sondern das passiert automatisch. Das ähm,\nAlso um ein Beispiel zu geben, das äh kann man sich ganz wunderbar runterladen bei Google. Das ist äh eine Vorstudie, die sie gemacht haben.\nUm Texte mit neuronalen Netzen äh bearbeiten zu können. Das heißt World World torektor, World zwei weg. Äh das ist ein,\nC-Programm glaube ich. Äh dem kannst du die Wikipedia oder Teile der deutschen Wikipedia vorschmeißen und erkennt,\nWeder die Sprache, noch um was es da geht erkennt nur die Textinformation. Der geht mit so einem,\nMit so einem Sliding Window geht ihr über den Text drüber immer fünf, sechs, sieben, acht Wörter gleichzeitig angucken und trainiert mit denen ein neuronales Netzwerk und nach ein paar Stunden oder paar Tagen,\nKannst du komplexe Fragen stellen wie äh Berlin verhält sich zu Deutschland,\nWie ähm Paris zu und dann kann es dir Frankreich beantworten und das äh ist was relativ Elaboriertes. Das hat es nur aus der Struktur gelernt. Das wusste nicht mal, welche Sprache das ist."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:39:14",
      "text": "Und jetzt auch ohne äh Berücksichtigung von äh wo ja genau diese Information relativ Maschinen lesbar vorliegt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:39:21",
      "text": "Genau, ohne das, also nur aufgrund der der beackert den Text lernt die Strukturfeatures des Texts,\nWeiß als erstes, ähm okay, hier sind wahrscheinlich Adjektive, dass sie sie die sehen aus wie Nominal-Phrasen, dann eine Ebene drüber lernt es äh,\nWie die immer aufeinander äh folgen. Und die ganze Grammatik fällt irgendwann raus aus diesem neuronalen Netz. Also."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:39:44",
      "text": "Aber so eine gewisse, also man irgendwas muss ja hardcodiert sein. Also sagen wir mal so, wie äh es gibt hier Sprache und ich muss diese Grammatik erst mal lernen, bevor ich in der Lage bin, aus diesen Daten irgendwas rauszunehmen. Das das muss ja da schon ein bisschen Teil der der Logik sein oder."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:39:57",
      "text": "Nee nee, das ist äh nicht mal das und das ist toll. Das Programm hat äh dieser Sliding Binden ist der die Optimierungsvorschrift da drin ist,\nGehe mit einem festen Window über den Text drüber Wort für Wort und dann weißt du äh dass Paris und,\nFrankreich oft in einem Satz vorkommen, wenn dein Sliding Window groß genug ist. Ähm und du trainierst mit diesem Sliding, wenn du der deutschen Wikipedie der Wikipedia das neuronale Netz in einem positiven Belohnungsschritt. Du sagst, das hier ist ein echter Satz.\nUnd jetzt nimmst du diesen diesen diese Sequenz und fängst an, fängst an randomly ein Wort rauszuschießen und durch Gabisch zu ersetzen.\nUnd sagst und das hier ist der nächste Lernschritt, das ist kein guter Satz.\nUnd dieser dieser einfachste Vorschrift befähigt das Netzwerk nach vielen Lernschritten solche Antworten zu geben oder die Strukturinformationen zu lernen aus dem Text.\nUnd zwar so gut, dass äh so Standardprogramme wie der Stanford, Natural Language Porsa, das Tool Kit, in dem glaube ich bestimmt 25 Jahre linguistische Forschung, Spitzenforschung drin stecken,\nDass die letztes Jahr angekündigt haben, dass sie ihren eigenen statistischen Pariser durch so ein neuronales Netzwerk ersetzen werden.\nDas heißt, selbst angesehene Linguisten sehen die die Qualität, dass das, dass sowas einfach besser funktioniert inzwischen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:41:22",
      "text": "Das heißt, der alte Ansatz von der Stanford University mit ihrem Langwidpasser war ja schaffen hier Kot, wo genug,\nkluge Menschen sozusagen ihr Metterwissen eingebracht haben mit hier das ist das was du dir anschauen musst. Das ist die die du sozusagen herausfinden musst und jetzt suchen wir uns mal Algorithmen dafür raus wie diese super Vektor wie hießen die noch gleich? Äh."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:41:44",
      "text": "Support weg damit."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:41:45",
      "text": "Pott Vektor."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:41:47",
      "text": "Ja, ich glaube, an der Stelle sind das keine Support weg, aber klar. Irgendein klassisches."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:41:49",
      "text": "Irgendwelche äh irgendwelche Körner äh äh Methoden sozusagen oder andere Algorithmen, die eben für den äh gewünschten Effekt geeignet zu sein,\nscheinen, aber trotzdem hat man immer noch so dieses dieses Metawissen und jetzt wollen sie das komplett ersetzen durch etwas, was eigentlich überhaupt von dem Problem gar nichts versteht, sondern was einfach nur Selbstlernen an diese ganze Sache herangeht und das alles von sich aus versteht."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:42:12",
      "text": "Genau, das das ist das, was sie was sie tatsächlich angekündigt haben. Die aktuelle Version, die man runterlädt, hat schon diesen diesen neuronalen Paser drin. Ähm,\nDer Vorteil ist, wenn du das mal akzeptierst, was erstmal schwer zu akzeptieren ist, dass so eine Maschine nur, indem sie den Text immer wieder liest, irgendwann elaborierte Strukturinformate mehr als als äh Strukturinformationen daraus schließen kann. Äh das heißt aber auch,\nDu kannst einfach einen beliebig neue Sprache nehmen und sagst, und jetzt verkaufst du diese Sprache und nach zwei Tagen fällt die Strukturinformation für diese Sprache daraus.\nUnd das ist natürlich was ganz Mächtiges, ne? Das heißt, du brauchst nicht mehr,\nein Spezialistenteam für Takalon und das nächste Spezialistenteam für Chinesisch, sondern du brauchst nur eine große Textmenge,\nUnd lässt den Rechner darauf rum äh kauen und irgendwann fällt es raus.\nDas ist äh das, was die was was man als äh an zu nennt. Also niemand steht an der Ecke und sagt der Maschine hier ist gut, hier ist schlecht und dann,\nMan hat eine relativ einfache Anweisung ein eingebaut mit diesem Sliging Window,\nUnd dann äh das ist scheinbar vollkommen ausreichend, um eine Grammatik aus einem aus einer Textmenge zu extrahieren.\nDas ist verdammt äh es ist auch ein bisschen das ist auch ein bisschen deprimierend. Das gebe ich oft, das gebe ich gerne zu."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:43:36",
      "text": "Deprimiert inwiefern man sich vorher so viel Gedanken über äh Algorithmen gemacht hat und jetzt geht's auf einmal so einfach. Obwohl's ja nicht im eigentlichen Sinne einfacher ist, man muss äh nun ja auch erstmal drauf kommen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:43:48",
      "text": "Man musste erstmal drauf kommen und ähm man brauchte die Rechenkapazität, die man, die man heute hat. Also als der hinten sein erstes PayPal rausgebracht hat,\nHat Google relativ schnell gesagt, okay, äh du kommst mal zu uns, wir brauchen dich. Wir haben viele Probleme, die dann auch schreien äh mit solchen guten Verfahren beantwortet zu werden und äh."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:44:11",
      "text": "Mit welcher rechten Kapazität hat er denn Feuer gearbeitet?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:44:14",
      "text": "Äh das weiß ich nicht. Ich nehme das mal an der Universität von Toronto auch irgend so eine Art von Supercomputer. Also irgendeine Art von universitären."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:44:20",
      "text": "Okay, also er hatte einfach mal die die Maschinen richtig klingeln lassen, um seine Thesen zu überprüfen und kam zu guten Ergebnissen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:44:25",
      "text": "Nehme ich mal an und äh Google hat ja wohl das ist jetzt so ein bisschen äh da muss ich weiß ich nicht so ganz genau, ob ich da zitierfähig bin, aber du hast gesagt, ich darf flapsig sein. Also ich glaube ähm,\nEr hat Google erstmal einen Korb gegeben und hat gesagt, ey ich habe ich ihr habt nichts, was äh was mich jetzt wirklich interessiert und dann haben sie ihm,\nViele viele CPU-Knoten angeboten für wirklich lange Zeit und eine gewaltige Datenmenge und er hat gesagt, okay, ich äh hätte gerne,\nAlle Videodaten, die im YouTube drin sind und ich trainiere einen universellen, visuellen Indikator für Katzen. Da sind wir wieder bei den Katzen und,\nMhm. Also es gibt tatsächlich einen äh ein Katzenindikator, der in jedem beliebigen YouTube-Video die Katze in Bewegtbild detektiert und sagt, da ist eine Katze drin in dem Video,\nDas gibt es."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:45:16",
      "text": "Wie richtig ist es."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:45:19",
      "text": "Wie richtig äh das ist richtig, richtig das ist äh von sehr hohe."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:45:25",
      "text": "Also er findet alle Katzen und er findet nichts, was keine Katze ist."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:45:28",
      "text": "Genau. Das äh in das, in den du das kann man sich bei bei YouTube, das ist eine schwierige Aufgabe, wenn man sich die Qualität und die unterschiedlichen Winkel der verschiedenen YouTube-Videos äh vorstellt.\nUnd Google äh ist natürlich sehr interessiert an dieser an dieser Art von Technologie gewesen, weil,\nDa gibt es noch diese andere große Datentonne, die sie haben. Da sind relativ viele Fotos der Erde drin und auf diesen Fotos sind Hausnummern und Gesichter und Autonummernschilder und,\nJetzt kann man sich die Frage stellen, wenn ich das anonymisieren will, dann äh jetzt mal abgesehen von,\nvon den von den Hauswänden, die ich nicht darstellen will, aber wenn ich alle Nummernschilder der Welt äh lesen, äh beziehungsweise auswischen will und alle Hausnummern der Welt in allen Sprachen der Welt übersetzen möchte, wo mein Kartenmaterial zu mappen, dann schreit das natürlich nach so einer Technologie.\nMan hätte gerne ein Netzwerk, was in der Lage ist, jedes beliebige Number Play zu erkennen, jedes Gesicht zu verwischen und tatsächlich ist es,\nEin solches Netzwerk, was da zum Einsatz kommt. Also da haben nicht Menschen gewischt, sondern das war ein neuronales Netzwerk. Und wenn man."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:46:37",
      "text": "Nicht auch schon vorher so, also das,\nja auch schon länger als 2tausend7, oder? Täusche ich mich jetzt gerade."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:46:46",
      "text": "Weiß ich nicht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:46:47",
      "text": "Das sind wieder so Fragen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:46:52",
      "text": "Also jetzt ist es wohl ihren Menschen etwas da erwischt. Vielleicht haben sie es vorher mit Menschen gemacht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:46:56",
      "text": "Okay Streetview selber ist 2tausend7 erschienen, aber dass sie das mit den äh Nummernschildern und so weiter gemacht haben, das kann man schon erst später."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:47:05",
      "text": "Also tatsächlich ähm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:47:07",
      "text": "Krass. Ich meine, das ist ja auch so ein bisschen der Tod von Captures, ne?\nAlso ich meine, wenn Captures versuchen, durch die Präsentation von irgendetwas äh zu beweisen, dass ich ein Mensch bin und dann habe ich irgendwie so eine Maschine, die das so dann noch sehr viel besser kann, dann äh ist es ja over. Also ich meine, man kann ja überhaupt gar nicht mehr beweisen,\nDass man äh ein Mensch ist, wenn die Maschine das viel effizienter und schneller genauso beweisen kann."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:47:31",
      "text": "Man braucht eine neue Art von Captures, ne, die eben Sachen abfragen, die tatsächlich im Augenblick nur Menschen können,\nUnd ähm so was wie ein Bild zu erkennen, das kann die Maschine eben gut. Ich meine, das äh davon sollte man auch ausgehen, wenn da draußen bald Autos selber rumfahren. Äh,\nUnd ähm sich ein Bild von der Wirklichkeit machen, dann möchte ich, dass die ein relativ genaues Bild von der Wirklichkeit haben und nicht irgendein verschwommenes Bild."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:47:59",
      "text": "Bevor wir äh vielleicht auf die gesellschaftlichen Implikationen zu kommen, weil die sind äh vielfältig. Ähm ich schon ganz gern noch mal so ein bisschen versuchen zumindest besser zu verstehen, wie man jetzt so diesen,\nVorgang sich im Einzelnen vorstellen muss. Ohne jetzt in irgendwelche Formeln und Matmaschen und Tiefen äh vordringen zu wollen.\nAber einfach noch mal, um so vielleicht so ein bisschen zu zerstößeln, weil so richtig vorstellen kann ich mir das nur sehr eingeschränkt.\nÄhm weiß nicht, ob jetzt dieses Textbeispiel, was du vorhin genannt hast, vielleicht ein guter Ansatz äh wäre wahrscheinlich schon, ne, bevor wir jetzt in den\noder,\num's noch mal so in etwas kleinere Schritte zu zerlegen.\nIch habe das jetzt ich habe da sozusagen jetzt ein Programm. Das kriegt jetzt die Wikipedia so von der Seite reingeschoben.\nUnd es schaut sich tatsächlich, was hattest du so acht, sieben irgendwie irgendein Fenster von Worten an gleichzeitig,\nEs schiebt immer ein Wort raus und guckt sie das nächste wieder mit an. Und es weiß einfach genau,\nGar nichts da drüber, außer dass es sich um Buchstaben handelt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:49:16",
      "text": "Um Wörter. Also es guckt sich es guckt nicht auf es gibt Programme die das auf Wort Level machen, da gibt's paar erfolgreiche Papers und auch Programme, aber das, was ich beschrieben habe, das guckt sich Wörter an.\nAlso tatsächlich geht das Programm als allererstes durch den ganzen Text durch,\nSepariert das Ganze nach Whitespace und nach Kommas und äh das tukenisiert sozusagen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:49:37",
      "text": "Also das einzige, das einzige wirklich gesicherte und vorgegebene Wissen ist Buchstaben, die Worte bilden,\nUnd Wort heißt halt, da wo Wicepace oder was auch immer dazwischen ist. Also es gibt irgendwie eine relativ klare Definition für was ein Wort ist.\nUnd äh woraus es sich zusammensetzt, das ist sagen wir mal das Einzige wirklich ins Nest gelegte äh Wissen an der Stelle."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:50:01",
      "text": "Genau, mehr ist da nicht drin."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:50:03",
      "text": "Aber es gibt noch das Programm weiß auch gar nicht, was eine Grammatik ist oder es weiß schon, dass es auch nicht."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:50:11",
      "text": "Weiß es auch nicht, nein."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:50:12",
      "text": "Und es weiß auch nicht, was eine Sprache ist."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:50:16",
      "text": "Hm wenn du jetzt, nee, das weiß ich nicht, also du ich meine, man könnte jetzt pingelig sein und sagen, wenn du äh wenn von Worten ansetzt, dann hast du vielleicht irgendwie doch was mit aber nein, das weiß nicht, was."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:50:28",
      "text": "Aber es ist nicht so, dass es da irgendwie so eine Variable im Dollar language äh äh gibt, die dann äh im Zeitraum des Programms dann mal gesetzt wird, sondern auch das ist sozusagen nur so eine Erkenntnis, die dem Programm selbst kommt.\nSo, jetzt kommen also diese Worte an. Ähm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:50:47",
      "text": "Die Worte, die."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:50:48",
      "text": "Die werden ja was was macht man jetzt mit diesen Worten?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:50:51",
      "text": "Also das man macht das was was so ein was solche Programme des maschinellen Lernens immer machen. Die verwandeln die Worte beziehungsweise die Wortketten als erstes Invektoren."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:51:01",
      "text": "Heißt was genau."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:51:02",
      "text": "Ähm du gehst durch den gesamten Text durch, zählst jedes einzelne Wort, hebst das auch auf und machst einen raus. Also,\nhebt jedes Wort ein Wörterbuch, ne? Zählst die Wörter, du willst natürlich nicht jedes Mal The auf äh heben, sondern es reicht, wenn man das einmal aufhebt. Vielleicht schreibt man noch dran, wie oft es vorkommt. Äh und dann hat man so einen von allen Wörtern.\nÄhm und jetzt geht man durch den Text durch, hat diese Sequenz von Wörtern, die gerade in diesem Sliding Window ist.\nKann jetzt sagen, okay, dieses Ding besteht aus, jetzt guckt man in das erste Wort, guckt im nach, ist Position,\ndreizehnhundert, also kommt an die Stelle in den Vektor in dreizehnhundert. Das zweite Wort ist im Dictionary Position Nummer drei, also kommt eine drei rein und du merkst es schon, irgendwann hast du diesen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:51:49",
      "text": "Position im Sinne von Häufigkeits-Ranking oder."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:51:52",
      "text": "Du kannst du kannst die Häufigkeit nehmen, du kannst auch die absolute Position nehmen, das ist eigentlich egal, die Hauptsache, das ist nicht egal, aber du äh die Hauptsache du machst es stringent. Du brauchst irgendeine Art von Ordnung,\nDu willst ein Wort in einen Zahlenvektor verwandeln und das äh passiert mittels. Das musst du musst es nur immer auf die gleiche Art und Weise machen.\nDann sind damit die Dinger irgendwie vergleichbar werden. Wenn du die das Verfahren in der Mitte änderst, dann äh wird das wahrscheinlich nichts."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:52:17",
      "text": "Okay, aber für The müsste er sich dann quasi, wenn's jetzt nur um die Position geht, jede Position merken."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:52:25",
      "text": "Äh ja, aber du bist ja in, also wir sind ja gerade bei diesem Sliding, wenn du an irgendeiner Position und das Wörtchen will, kommt da einmal drin vor."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:52:32",
      "text": "Also innerhalb dieses betrachteten Fenster."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:52:34",
      "text": "Innerhalb dieses du willst nur dieses Fenster gerade in einen Vektor verwandeln und ähm das machst du, indem du eben jedes einzelne Wort das aus dieser Sequenz nimmst und durch den Zahlenwert aus dem ersetzt. Und jetzt hast du stattdessen,\nZahlen in einer Zahlen, das weiß man schon, dass es für Computer immer einfacher als äh ne so.\nUnd diesen Vektor, den den wirfst du jetzt dem dem neuronalen Netz zur Fraße vor, in seinen Eingangsneuronen. Die kriegen jetzt diese Zahlen geliefert. Diese Zahlen, die dem diesem Sequenz, dieser Sequenz entsprechen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:53:08",
      "text": "Jedes einzeln oder dann alle auf einmal."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:53:10",
      "text": "Alle auf einmal. Es."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:53:12",
      "text": "Also erstmal läuft man alles durch die Styling Window, Textblöcke."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:53:17",
      "text": "Äh nee, nee, Entschuldigung, also da in dem in dem Falle Stück für Stück. Also man geht an das Sliding Window hat Positionen irgendwo ganz vorne."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:53:25",
      "text": "Der betrachtete Bereich wird zu seiner Zahnkolonne und die wird in das Netz gegeben."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:53:30",
      "text": "Die wird in das Netz gegeben und dann."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:53:32",
      "text": "Was heißt das? Es wird in das Netz gegeben. Was passiert dann da?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:53:36",
      "text": "Die na ja die also das neuronale Netz ist natürlich irgendeine Art von Variablen sind da, die die Werte aufnehmen und mit diesen Variablen macht das Programm was. Also du gibst die ersten Zahlen in die in diese dieses variablen Areay rein,\nund sagst, das ist der, das ist der Wortvektor,\nUnd jetzt ähm weiß das Netzwerk noch gar nichts. Das ist irgendwie, das sind verschiedene Schichten von Neuronen, aber es werden oben immer weniger."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:54:04",
      "text": "Wie viel Neuronen sind das so?\nAlso größenordnungsmäßig."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:54:10",
      "text": "Das kommt auf die Aufgabe an."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:54:12",
      "text": "Ja, bei dieser Aufgabe, was würdest du so erwarten?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:54:16",
      "text": "Hm ein paar hundert."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:54:18",
      "text": "Ein paar hundert Neuroden, die alle was für eine haben die einen also wie bildet man so einen Neuronen ab? Hat das einen,\nWert oder hat das nur Verbindungen? Hat das einen Zustand."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:54:34",
      "text": "Das hat einen Zustand und es hat Verbindungen und diese Verbindungen sind ähm also jedes Mal rund verbindet sich mit anderen Neuronen. Das äh ist ja im Kopf auch so und das hat verschiedene Verbindungen. Die sind hier in diesem Fall fix. Also sind,\nÄh und jeder dieser Verbindungen hat ein Gewicht, einen Zahlenwert.\nUnd die werden am Anfang diese Zahlenwerte, das sind die wichtigsten, das sind die Zustandsgrößen des äh neuronalen Netzes, also das neuronale Netz besteht im Prinzip aus der Beschreibung ihrer Struktur und diesen Werten. Das ist es.\nDiese Gewichtswerte aufschreiben und wo sie hinkommen natürlich und dann findet,\nmeistens so eine Art. Das ist ja, es findet eine Art Datenreduktion statt, also oben gibt es am Ende einen\nEin ein Entneuron, was dafür zuständig ist zu sagen, das ist ein echter Satz, den Menschen geschrieben haben, weil er aus der Wikipedia kommt,\nUnd es gibt ein zweites Neuronen, was sagt, das ist ein Satz, in dem Garbage drin ist. Das ist das, was ich vorhin beschrieben hatte, indem man ein Wort ausfügt und,\nÄhm man gibt also unten in das neuronale Netzwerk diese Zahlenvektoren des Liding Windows rein, einmal das echte Beispiel und einmal das schlechte,\nWeißt den Zustand der oberen Neuronen, was man haben möchte. Ne, einmal soll das linke Neuron, echter Satz gefeuert werden und einmal soll das Rechte gefeuert werden."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:56:02",
      "text": "Wobei, wenn man jetzt ein Wort weglässt, kann er da trotzdem noch ein normaler Satz bei rauskommen. Ist ja jetzt auch nicht so gesichert."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:56:07",
      "text": "Kann es, kann so sein, ja, aber man geht davon aus, dass es äh es ist ein statistisches Verfahren, dass es öfter kein ordentlicher Satz gibt,\nals negativen Lernschritt und dann rechnet man von diesem aus dem oberen sagt okay unten liegen die Werte, oben weiß ich schon, das ist entweder das linke oder das rechte Neuron, das was ich haben möchte und dann lässt man von oben das Netzwerk zurückrechnen.\nRechne die Koeffizienten aus, bis es bis alle Zustandsgrößen so haben, so so so sind, wie ich sie haben möchte. Das ist das Back Propagation.\nDas geht jedes Mal in ganz kleine Schritten. Man justiert jeden Wert nur ein ganz kleines bisschen. Dafür macht man das ganz oft immer wieder. Man,\nLernt, ein paar tausend Mal in ganz, ganz kleinen Schritten. Und irgendwann hat dieses Netzwerk einen Zustand angenommen, der sich kaum noch verändert. Dann sind die ganzen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:56:58",
      "text": "Weil sich sozusagen im im Rahmen dieser Back Propagation verändern sich dann eben auch die Vernetzung zwischen den Neuroden, weil sie merken,\nÜber diese mögliche Verkettung äh erziele ich mir positive Signale, über die mehr negative Signale, deswegen verstärke ich jetzt diesen Bereich.\nDas richtig, also\ndieses Training ist sozusagen dann eine eine Stärkung von existierenden Verbindungen oder vielleicht auch Schaffung von äh Verbindung überhaupt erst mal.\nUnd das Öl von unten nach oben wirken ist sozusagen erstmal die unmittelbare Auswirkung und dieses von oben nach unten ist mehr so die die diese,\nVerstärkung des gemessenen Werts."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:57:41",
      "text": "Ja, ja. Doch. Schön. Gute Zusammenfassung. Im Grunde ist es."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:57:46",
      "text": "So müsst ihr mehr nutzen, die andere."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:57:48",
      "text": "Im Grunde ist es das."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:57:49",
      "text": "Mhm.\nOkay, wenn ihr dann wird das auch relativ klar. Das heißt, diese diese Worte äh pulsieren jetzt sozusagen ein, man man zerschießt sie sozusagen, also man nimmt das Ergebnis und sagt, hier ist X und hier ist kaputtes X.\nUnd dann lässt man das so kurz äh reinflackern,\nUnd äh äh man sieht irgendwie, ah äh wenn wenn Heiles X dann dann drückt sich das so aus und und wenn kaputt das X dann drückt sich das so aus und und und das,\nDie positive Auswirkung, die wird dann quasi wieder reingeschrieben in dieses Netzwerk von oben nach unten. Und bleibt dann als Gelerntes Moment."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:58:29",
      "text": "Genau und das machst du jetzt für jeden für jedes Wort für jeden Wortblog immer weiter. Das sind viele, viele Lernschritte.\nUnd irgendwann sind ist das Netzwerk in einem irgendwann ist das Netzwerk in einem Zustand, wo alle Koeffizienten sich kaum noch verändern. Da kannst du den Text noch so oft durchjagen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:58:46",
      "text": "Wie dimensioniere ich denn die Anzahl der Neuronen, die ich da an der Stelle verwenden will? Ist da äh viel hilft viel oder?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:58:54",
      "text": "Nee, nicht unbedingt. Du machst das von deinem Problem abhängig, wenn du ähm jetzt sagst, ich hätte gerne ein Sliding Window von sieben Neuronen, dann ist dein, ist deine Eingabe-Level sieben Neuronen.\nBreit, ne, weil das so so breit ist dein Fenster. Es macht nicht viel mehr Sinn, was wenn du links und rechts noch welche hast, die du nie benutzt, dann musst du die lässt du die schon weg in der."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:59:15",
      "text": "Ja gut, aber ich meine auch die Definition, was ist meine Eingabe? Also."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "1:59:19",
      "text": "Da wird einfach viel probiert. Das äh das,\nDa wird, ja, das wird das wird schlicht und einfach wirklich ausprobiert. Man, es gibt ein paar Leute, die die sehen das schon, weil die das öfter gemacht haben. Die gucken in der geeigneten Art und Weise hin, aber,\nIm Grunde stelle ich mir das oft so vor, dass da irgendein Professor an einem weißen Tisch sitzt und der sagt, seinen\nStudenten. Okay, jetzt hätte ich gerne das ausgerechnet und das sollte auch mal jemand probieren. Das ist doch sehr sehr schön und ein Jahr."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "1:59:45",
      "text": "Fehlen da immer so viel. Also im Prinzip dann Evolution. Das ist sozusagen wie sich,\nKörper eben diese äh Neuronenmengen auch äh den Bedürfnissen angepasst haben.\nÜber die Zeit, so muss man das halt jetzt äh beim Machine-Learning mit diesen neuronalen Netzwerken eben einfach durch arkumuliertes Wissen,\nSagen okay, hier braucht man mehr, um bessere Ergebnisse zu erzielen, aber wir müssen es jetzt auch nicht verdoppeln, weil das heißt, dass wir 30.000 mal so lange rechnen und das bringt uns auch nicht wirklich weiter."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:00:13",
      "text": "Genau, also man man kann mathematisch relativ gut ausrechnen, wie viel Neuronenschichten man braucht, um ein\nProblem zu lösen, wenn man das Problem gut verstanden hat, das hat man oft nicht deswegen nimmt man ja neuronale Netze, also wenn man weiß,\nwie viel Dimensional das Problem und wie nicht linear das Problem ist, was ich lösen möchte, dann kann ich die Neuronenschicht relativ einfach beschreiben, aber oftmals weiß ich genau das nicht, deswegen arbeite ich mit einem neuronalen Netzwerk."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:00:39",
      "text": "Ist schon ein bisschen absurd, dass man jetzt irgendwie feststellt, dass man äh genau dann ein Problem besser verstehen kann, wenn man mit einem System daher kommt, was das Problem überhaupt nicht kennt. Ja."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:00:51",
      "text": "Das ist irre, ne? Also vor allen Dingen, wenn man so wie wir eher schon zu den alten Hasen gehört und Jahrzehnten Jahrzehnte."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:00:58",
      "text": "Ja eh alles. Und vor allem besser."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:01:02",
      "text": "Und Jahrzehnte damit zugebracht hat, äh mit diesem alten Paradigma zu leben. Okay, man muss das Problem für die Maschine lösen und plötzlich steht da jemand und sagt, nee, wenn du das ordentlich formulierst und das Ergebnis auch, dann löst das die Maschine für dich. Also das erste Mal,\nSchreibst du nicht das Programm und die Maschine führt das für dich aus, schnell und gut und sicher, sondern aus der Maschine fällt das Programm raus, was das Problem löst. Das ist schon,\nEine andere Nummer, ne? Und wir wir reden."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:01:30",
      "text": "Wir dachten, wir hätten einen sicheren Job."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:01:32",
      "text": "Und jetzt sind wir Datenpfleger oder äh ich meine, wenn man Kinder hat, dann ist einem das vielleicht nicht so ganz fremd. Dann muss man einfach bestimmte Sachen muss man 30 Mal erklären und nächstes Jahr nochmal und äh weil dann haben sich die Sachen angepasst."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:01:47",
      "text": "Ja, irgendwann ist man kein Programmierer mehr, sondern ein ein Gießband, nur das neuronale Netz.\nHu.\nAber jetzt, jetzt muss ich noch mal, doch noch mal kurz auf dieses ähm weil es gibt ja offensichtlich eine Menge her, weil wir haben ja im Prinzip nur die erste Stufe jetzt gemacht. Also diese Wortkette kommt hereingeschossen,\nUnd ich habe jetzt verstanden, wie sich dieses Netz trainiert. Was ich noch nicht verstanden habe, ist, wie man da jetzt sozusagen auch eine eine Aussage draus ableiten kann. Also es verstärkt sich halt quasi eine Vernetzungsstruktur.\nUm Dinge,\nZu beantworten, aber eigentlich ist ja die einzige Antwort, die sich bisher dort wirklich abbildet, ist ja sozusagen nur die dass es etwas, was dadrin vorkommt oder das ist etwas, was da nicht drin vorkommt. Das ist im Prinzip so die einzige wirkliche Information jetzt ab\nAbgesehen von den Zahlen selbst. Wie kann man dann daraus."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:02:42",
      "text": "Wo es wo ist die Grammatik?"
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:02:44",
      "text": "Ja genau, also ich meine wir hatten ja sogar gesagt, es weiß ja noch nicht mal, was es ist und und und weiß auch nicht, was was verstanden werden soll und auf einmal kann es irgendwie äh sowohl sehr wo kroatisch äh als auch altgriechisch äh interpretieren."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:02:58",
      "text": "Also irgendwo die Information muss im Netz sein. Sie ist also höchstwahrscheinlich in diesen Koeffizienten verborgen und äh wir haben ja schon gesagt, es sind mehrere Schichtenebenen des Netzes, es ist Deep-Learning, also es ist mehr als eine Schicht.\nUnd tatsächlich kann man oh Gott, ich bin nicht ganz sicher, wo die Post-Text rausfallen. Ähm,\nDie Grammatik ähm wo das Netzwerk gelernt hat, ob etwas ein ähm eine nominale oder eine verbale Phrase ist, die findet sich auf Schicht drei, glaube ich.\nAuf Schicht vier. Also man guckt einfach in den."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:03:34",
      "text": "Zwischen einer nominalen äh verbalen Phase."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:03:37",
      "text": "Eine nominale Phrase, es ist die das ist das der Hauptwortteil und die verbale Phrase ist das äh wie sagt man Teil? Äh also ich,\ngenau das Verb, ne, nominal und äh verbade Phrase. Das ähm also in der Linguistik sind das die die ähm,\nTeile, die einen Satz ausmachen, ähm die zerfallen dann noch mal in Adjektive, Nomen und so weiter und äh diese Teile,\nfallen auf bestimmten Ebenen des Netzes automatisch raus. Ich gucke mir einfach drei an, also der unterste Layer ist äh ich kann ihn null oder eins nennen. Da wo die Worte wirklich reinfallen,\ndann drei Layer weiter höher, sind auf einmal ist auf einmal gelernt worden, was Adjektive und ähm,\nNomen sind und wieder eine Ebene drüber, fällt eine komplexere Teil der Grammatik raus. Also man guckt in den verschiedenen Ebenen nach.\nVersteht jetzt einfach okay, da bildet sich das ab lustigerweise. Also man hat das Problem gespannt, aus der untersten Ebene das wirkliche Wort und der obersten Ebene, wo man noch gesagt hat, echter Satz oder nicht echter Satz. Und dazwischen.\nEntspannt sich auf den verschiedenen Ebenen verschiedene Teilaspekte der Grammatik dieser jeweiligen Sprache und das ist ziemlich verrückt,\nEs heißt aber einfach nur äh menschliche Sprache, genügt bestimmten Regeln. Diese Regeln,\nFormen sich in ihrer Struktur aus und wenn ich ein Netzwerk trainiere äh auf die Struktur einer Sprache und das verändert sich irgendwann nicht mehr,\ndann hat es offensichtlich die Struktur komplett erfasst und dann kann ich, wenn ich mir bestimmte Ebenen dieses Netzwerks angucke, auch finde ich auch bestimmte Aspekte von meinem von meiner Grammatik wieder heraus.\nWenn sie denn stimmt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:05:33",
      "text": "Hm ja ist ein bisschen verrückt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:05:37",
      "text": "Es gibt einen,\ntotal schönes Beispiel. Im YouTube, ich weiß nicht, das kann man bestimmt verlinken oder sowas, das heißt ähm inside and artificial brain oder inside and artificial Euro Network äh da,\nIst das Google, Image Net, was Sie benutzen, um ähm,\nWas sie meiner Meinung nach benutzen, um äh Nummernschilder zu erkennen und Gesichter auszuwischen,\ndas haben sie trainiert und sie fahren in diesem Film alle paar Sekunden durch eine neue Schicht dieses neuronalen Netzes durch und man kann ihm zugucken, wie es von einfachem Rauschen,\nüber Katzenbilder, über Dreiecke und alles mögliche lernt. Das ist total hübsch. Ähm."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:06:22",
      "text": "Ich sehe grad, Punkte werden zu Linienflächen, Strukturen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:06:27",
      "text": "Genau, das geht jetzt eine ganze ganze Schritt weiter, denn also unten war das äh war das Rauschen sozusagen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:06:32",
      "text": "Ja, genau."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:06:33",
      "text": "Und jetzt wird's immer komplexer. Erinnert auch so ein bisschen an fragtaler das liegt daran, dass das äh so aus der Mitte ähm iterativ sich aufbaut. Das ist aber nur ein Trick, um es äh grafisch interessant zu machen. Das ist schon,\nauthentische Fahrt durch so."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:06:49",
      "text": "Ist das fett.\nOkay, wow. Unglaublich."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:06:57",
      "text": "Also irgendwann kommt man an diesen Effekt, äh wenn man mal ein,\nBewusstseinsverändernden Substanzen zu tun hatte, dann weiß man relativ genau, irgendwann erkennt man das wieder. Da ist so eine Schicht und dann äh sieht man wow, da das muss doch ein recht gutes Abbild äh von unserem visuellen System sein.\nIch weiß nicht, ob du da schon bist. Ich sehe nur den Rücken deines Laptops."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:07:18",
      "text": "Äh ja ähm weiß ich jetzt auch nicht, ob ich da jetzt da äh bin, aber das sieht ja alles äh recht äh trrippig aus, aber versuche das gerade so ein bisschen hier zu äh Pasen, während ich beim Basen\nZuschauer aber äh ja sehr interessant, ne also äh wirklich so ein bisschen so der der Eindruck, dass es hier vom Zellulären,\nAlso vom vom atomaren ins Zelluläre äh übergeht und ähm ja jetzt immer mehr äh Struktur erhält. Meine Fresse."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:07:48",
      "text": "Irgendwann kommen die irgendwann kommen die Augen. Das."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:07:51",
      "text": "Ja, jetzt sieht's schon so nach Augen aus, ja. Mal das richtige Augen oder nur etwas, was ich schon als Auge interpretieren würde."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:07:58",
      "text": "Oh nee, da kommen irgendwann richtige Augen und irgendwann kommen auch die Katzen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:08:02",
      "text": "Oh und äh weißt du jetzt gerade auf was diese ähm Daten, also was für Daten jetzt hier sozusagen gerade analysiert werden. Also das sind dann die Katzenbilder. Ich sehe schon Hunde und so."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:08:12",
      "text": "Äh ja. Also nee, ich weiß, das Trainingsset von dem kenne ich nicht. Das kann man bestimmt irgendwo nachlesen, aber das weiß ich nicht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:08:19",
      "text": "Aber das muss ja jetzt sozusagen damit was zu tun haben, weil die Augen jetzt sozusagen herkommen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:08:23",
      "text": "Es wird ähm es wird ja huh Quatsch, es wird es wird äh YouTube sein, die kommentierten Geschichten. Es wird wahrscheinlich eine ganze Menge aus dem Google Image-Pool sein, der ja auch gelabelt ist.\nDen wir ja eine ganze Zeit lang mitgelabelt haben, indem wir irgendwelche blöden Fragen beantwortet haben für Google und das wird das Trainingsset sein für dieses, mit dem das Netz trainiert worden ist."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:08:48",
      "text": "So, Carry.\nKann man sich echt beobachtet vor Finster. Oh Mann.\nOkay, ähm nochmal versuchen, das ein bisschen äh zusammenzufassen. Wo wo's bei mir noch so ein bisschen aushakt ist sozusagen, wo man jetzt auch selber als ähm,\nPfleger dieses Datenwaldes äh und dieser ganzen Analysenstufe auch äh entsprechende äh Aussagen dann Ab\nKann. Also ich meine schön, dass wir da so ein so ein so ein so ein haben, der äh offensichtlich so auf alles äh reagiert und da Aussagen zu machen, aber ich muss ja nur noch irgendwo dann mal eine konkrete Frage ab\nKönnen und muss das ja auch wirklich äh dann zu meinen Zwecken ähm bedienen können. Wie wie wie steige ich denn in dieses Verständnis der das ja nun zunächst einmal nur in diesem Neuronensystem selber drin ist? Wie wie mache ich das für mich nutzbar?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:09:47",
      "text": "Du meinst wie du willst nicht auf eine Anwendung raus, ne? Du."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:09:51",
      "text": "Na ja, nee, ich meine, wenn wir jetzt mal bei diesem Textbeispiel äh bleiben, jetzt habe ich also da diese ganzen,\nTextrings äh durchgefüttert und da ist jetzt äh keine Ahnung, jede jede Variante von Wikipedia drin und ich habe irgendwie so das Gefühl, jetzt äh habe ich da so einen Datenpool, der im Prinzip alles weiß.\nÄh wie mache ich jetzt dieses Wissen für mich urba?\nSage ich übersetze etwas da rein oder sage mir äh welche Sprache das ist oder äh,\nvollziehe nach, eine, eine, eine Aussage über ein bestimmtes Objekt\nDiese Datenpunkte müssen sich ja mir irgendwie auch erstmal in irgendeiner Form offenbaren, damit ich auch in der Lage bin, dafür einen User-Interface zu äh kreieren."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:10:33",
      "text": "Ja, die."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:10:35",
      "text": "Oder fängt das Ding für dich an zu denken."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:10:37",
      "text": "Nee, das hängt nicht an zu denken. Das hat äh das hat wie gesagt nur Strukturinformationen gelernt und das weiß ja nicht mehr, was es da gelernt hat. Das hat es bildet auf eine komplexe Art und Weise die Strukturinformation eines Textes ab. Äh,\nUnd was wir reingegeben haben, ganz am Anfang sind Zahlenvektoren, die diese Zahlenvektoren entsprechend diesen Wortsequenzen, aber äh am Ende habe ich,\nEinen großen Vektorraum mit lauter Datenpunkten drin. Und in dem kann ich Fragen stellen. Also ich kann sagen.\nGib mir alle Worte raus, die die größte Ähnlichkeit zu Plus haben als Wort. Und tatsächlich ist die erste Antwort, die das äh,\nNetzwerk liefert. Äh ich habe hier aber Positive habe ich gefragt. Das hat alle sehr überrascht und die erste Antwort, die kommt, ist äh non negative.\nUnd da merkt man schon, das ist nichts, was äh nach ähnlichen Worten sucht, sondern es findet tatsächlich die Bedeutung des Non-Negative-Positive bedeutet,\nIch habe das äh meinen äh Mathematiker Kollegen mal gezeigt und das ist äh da stellt man dann sofort ganz viele mathematische Fragen und es kommen kluge Antworten, aber,\nPositive und hat die meisten äh überzeugt, dass das äh was ist. Also man kann nach nach Ähnlichkeit fragen,\nUnd weil man in einem Vektorraum ist, kann man sogar mit diesen Worten bestimmte geometrische Operationen äh man kann äh Vektorverschiebung machen, die dann äh also ne, eine geometrische Operation, die man noch kennt aus der Schule,\ndie dann auf den Wörtern auch etwas macht, was tatsächlich Sinn ergibt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:12:15",
      "text": "Es musst du halt noch mal gucken hier ähm dein Beispiel, was ich hier gerade rausholen soll. Ich weiß gar nicht, ob ich das richtig gemacht habe. Äh Google hat doch dieses Deep Dream."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:12:27",
      "text": "Ja, das ist."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:12:28",
      "text": "Das meintest du eigentlich. Oder das ist nochmal was anderes."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:12:31",
      "text": "Also Deep Dream ist auch ein äh ein äh Abbild des visuellen Cortex. Also das ist was, was sich mit optischen Sachen beschäftigt. Was ich vorhin beschrieben habe ähm ein Teil davon ist World to Vektor."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:12:44",
      "text": "Mhm."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:12:45",
      "text": "Das ist so ein C-Programm, ähm was tatsächlich gar nicht die macht, aber das äh,\nwäre zu spitzfindig zu erklären. Es ist ein neurolales Netzwerk am Ende innendrin, dass eine Transformation von Wörtern in einen Vektorraum vornimmt und in diesem Vektorraum sind ähnliche Worte äh in einem gleichen Raumbereich."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:13:07",
      "text": "Was hat jetzt äh Google mit diesem Deep Dream äh konkret gemacht."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:13:11",
      "text": "Ist der also ich habe ein Netzwerk erfolgreich trainiert auf viele, viele Bilder ähm und bin irgendwann fertig. Ich habe definiert mehr trainiere ich nicht. So ist jetzt ist das Netzwerk fertig und jetzt kann man die Frage stellen,\nWas passiert, wenn ich in die in die Eingangsschicht dieses visuellen Netzwerkes kein Bild reingebe, sondern Bildrauschen? Was kommt dann raus?\nUnd dann, das haben die Leute, die das PayPal geschrieben haben, als Träumen definiert. Also ich gebe in die Eingabeschicht, gebe ich Bildrauschen rein, zufälliges statisches irgendwas und gucke mir dann an, was in den verschiedenen Layern des Netzwerkes passiert.\nUnd das nennen sie dann Träumen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:13:51",
      "text": "Mhm, verstehe."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:13:52",
      "text": "Träumen artifizielle Netzwerke von."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:13:55",
      "text": "Elektrischen Schafen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:13:56",
      "text": "Von elektrischen Schafen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:13:57",
      "text": "Und."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:13:58",
      "text": "Eher von elektrischen Katzen, die die Schafe,\nAlso Schafe sehe ich tatsächlich ganz selten irgendwie auf dem untersten Löcher sind vielleicht mal ein paar aber die meisten sind tatsächlich Katzen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:14:10",
      "text": "Ja gut, das hängt ja natürlich dann auch noch mal so ein bisschen davon ab, was man sich äh äh vorher alles angeschaut hat."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:14:16",
      "text": "Klar, also das äh du träumst nicht von Sachen, die du noch nie gesehen hast."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:14:20",
      "text": "Ja ja, genau. Jetzt,\nWas was ergibt sich äh daraus für uns? Also ähm Google hat sich den Herrn hinten schon mal,\nEr hat ihn gleich in so einen Glaspalastpalast äh reingestellt und der darf da jetzt sozusagen auf allen Knöpfen drücken, die irgendwie möglich sind. Jetzt wissen wir ja schon, dass Google nicht nur Google, aber,\nauch und vor allem Google ähm ja nennenswerte Datenmengen,\nzur Verfügung hat. YouTube war schon ein schönes Beispiel, alle Videos, Fotos äh,\nist auch ganz klar, äh Selbes kann man sich natürlich auch mit Facebook äh äh denken. Wenn man diese Datenbestände\ndie heute schon existieren und die äh mittelfristig sich noch weiter abzeichnen.\nIch denke, das wird ja tendenziell erstmal nicht weniger werden. Egal wie viele Gegenbewegungen es da gibt. Wir äh knipsen und schreiben und und und schicken Nachrichten\nMan hätte jetzt sozusagen wirklich den Zugriff auf alle Daten.\nNoch nicht mal auf ganz richtig alle Daten, aber so also viel der Kommunikation, viel dessen, was öffentlich geteilt wird,\nUnd man setzt jetzt,\nBeliebige Rechnermengen mit diesen die Algorithmen auf diese Daten an. Meine wenn,\nWenn wenn ein System schon,\nAnfangen kann zu übersetzen und Aussagen zu machen bis hin zu mathematisch vollständig, nachvollziehbaren Aussagen über etwas, wo vorher noch nicht mal ein Wissen darüber existierte.\nKann man ja da einfach alles Mögliche herauslesen. Kann man alle Beziehungen zueinander herauslesen,\nAlle Personen herauslesen und auch irgendwie rauslesen, was alle denken."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:16:20",
      "text": "Ja, das weiß ich nicht. Ich glaube, ich ich mag da nicht dran glauben ähm vielleicht will ich dann meinen Job loswerden äh."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:16:30",
      "text": "Du meinst, weil du dann auch durch ein Programm ersetzt wirst. Ja, das könnte ja eh passieren."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:16:36",
      "text": "Nee, ich glaube, die das, was uns ausmacht,\nDass das noch nicht verstanden ist. Vielleicht ja, also es gibt eine ganze Menge Leute, die sagen, nee, das ist verstanden. Das ist auch unnötig. Dieses Selbstbewusstsein und die Fähigkeit zur Selbstreflektion, also vor allen Dingen das Selbstbewusstsein.\nIst wahrscheinlich unnötig oder ist eine Grundeigenschaft eines beliebigen, komplexen, neuronalen Netzwerkes,\nDas muss sich selbst irgendwann die Geschichte die Geschichte erzählen. Du bist du bist da und am Leben, damit es gut funktioniert.\nAber im Grunde zum Funktionieren ist es nicht nötig. Zum Reagieren, zum Reagieren. Ähm,\nIch bin nicht der Meinung, dass das so ist. Ich glaube, das dauert noch ein bisschen. Ich weiß nicht, wie lange, aber."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:17:22",
      "text": "Aber was machen denn diese diese Techniken? Ich meine bis jetzt ist das seit 2007 im äh in der Welt. Jetzt sind ja schon acht Jahre vergangen, wo begegnet uns denn das schon?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:17:33",
      "text": "Die das begegnet uns, wenn wir unser Mobiltelefon, was meistens sehr wahrscheinlich ein Smartphone ist äh anschalten und entweder hey Google oder hey Siri sagen ähm,\nOkay Google, okay Google oder hey Siri äh."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:17:50",
      "text": "Wenn man wenn man, wenn man okay Siri und hey Google sagt, dann explodiert alles."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:17:53",
      "text": "Okay, verdammt."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:17:57",
      "text": "Dann begehen die Maschinen selbst Board. Das ist unser Vollback.\nAktiviert wurde, müssen wir den ganzen Tag hey Google und OK Siri sagen, dann sind wir wieder befreit. Das mal so als Tipp nebenbei. Okay, aber dieses Systeme, was machen die? Die arbeiten so."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:18:17",
      "text": "Die arbeiten so. Die sind irgendwann, man ein bisschen länger ähm mit,\nSprache zu tun hatte oder automatischer sprachlicher Übersetzung ähm ist schon so ein älteres Häschen ist, dann weiß man, das hat eine ganze Zeit lang recht gut funktioniert, aber aber irgendeinem Punkt ist das mit der automatischen Spracheingabe immer ziemlich Hamburgs gewesen.\nÄhm und da hat sich,\nirgendwann in den letzten fünf, sechs Jahren was getan, wenn man das einschaltet, merkt man, das funktioniert richtig gut, auch an der lauten Straße und zwischen,\nanderen Tönen aufgestellte äh Fragen werden gut verstanden und das liegt,\nAn genau der Technologie, die über die wir uns lange mit dem visuellen Cortex unterhalten haben, das funktioniert genauso mit dem Audioteil des Gehirns. Das ist auch ein tiefes neuronales Netzwerk, was selber lernt, zu hören,\nUnd ähm diese Technologie sind in Google genauso wie in Siri drin."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:19:16",
      "text": "Das aber es ist wirklich so. Ich meine mich begleitet Spracherkennung,\nUngefähr seit 20 Jahren. Vor 20 Jahren habe ich das erste Mal auch selber in einem Projekt versucht, Spracherkennung zum Einsatz zu bringen. Und es war halt einfach nichts anderes als frustrierend. Also es war einfach,\nvollkommen klar, dass das,\nGut, man kann damit Kreditkartennummern irgendwie übers Telefon eingeben, so mit dieser Charmans, mit der man gerne mit Computern äh redet und fünf, neun, acht."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:19:48",
      "text": "Meinten Sie fünf."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:19:51",
      "text": "Sag's, genau. Äh und auch als das so mit ähm,\nMit dem Google Now und mit mit Siri losging, standen am Anfang auch entsprechend frustrierende äh Ergebnisse, wo auch dann interessanterweise so eine Abneigung auch entstand,\nSich überhaupt mit dem Computer unterhalten zu wollen. Also es es war immer sehr man man hat ein kaltes Gefühl gehabt und,\nich war da auch immer sehr. Ich ich wollte das auch nicht. Einfach aus aus diesen Erfahrungen heraus weil man diesen Rechner,\nirgendwie als was anderes, als was was Kaltes, als kein Gegenüber äh mir ging's zumindest so. Ich hab's nicht akzeptiert. Ich wollte nicht mit meinem Rechner reden."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:20:40",
      "text": "Ich habe irgendwann auch auf Frustration aufgehört. Ich habe das Gefühl, mit dem Kalten nicht gehabt, aber ich habe immer gedacht, warum soll ich dieses Interface benutzen, was sowieso nicht vernünftig funktioniert und so habe ich das abgespeichert bei mir."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:20:50",
      "text": "Genau,\nVielleicht war das mit mit dieser Kälte gar nicht so sehr der Primärgrund, sondern erstmal nur funktioniert nicht, was letzten Endes wahrscheinlich auf das selbe hinausläuft, weil wenn man sich mit jemandem unterhält, der einen auch nicht versteht, dann entsteht ja genau diese Kälte.\nUnd wie du sagst, irgendwas ist besser geworden. Also ich würde es jetzt mal,\nAuf die letzten zwei Jahre verorten irgendwo, das sind alles fließende Übergänge,\nDie vermutlich die Unternehmen damit verbracht haben, einfach noch mehr zu trainieren, noch mehr zu trainieren, noch mehr zu verstehen, noch mehr Neuronen auszubilden."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:21:27",
      "text": "Tatsächlich überhaupt ähm das Verfahren auszuwechseln. Also da sind ähm andere."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:21:33",
      "text": "Weil sie am Anfang noch gar nicht da drauf gesetzt haben in der Form. Das heißt jetzt leben wir quasi in der Zeit der äh Hintenrevolution. Kann man das so sagen?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:21:42",
      "text": "Kann man so sagen. Das äh wir unterhalten,\nWir verbringen jeden Tag damit, uns mit neuronalen Netzen zu unterhalten, wenn wir Facebook anmachen, da laufen die ganze Zeit die learning Netzwerke hinter uns, die optimieren und\näh schalten und walten, wenn wir Siri anmachen, wenn wir automatische Bild erkennen, wenn wir unser selbstfahrendes Auto aus der Garage lassen, was wahrscheinlich,\nDa wir sind äh."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:22:08",
      "text": "Der Effekt ist sehr interessant, weil ich jetzt auch zuletzt,\nDas war auch so ein ein etwas unwirklicher Moment, muss ich sagen, ja, wo man einerseits das Gefühl hatte, so, oh,\nDas System hat wirklich verstanden, was ich gesagt habe, so und auf einmal war auch diese Kälte weniger.\nWürde noch nicht sagen, dass ich vollständig warm geworden bin damit, so, aber die Raumtemperatur ist definitiv angestiegen.\nSo auf auf einmal entsteht eine eine Nähe, die irgendwie äh schon fast ein wenig äh ist, aber sie ist halt nun mal da, weil sich einfach in mir weniger dagegen wehrt.\nUnd ähm das äh,\nJa, das das gibt einem zu denken. Ich meine, deswegen unterhalten wir uns ja jetzt hier auch einfach mal über dieses äh Thema, weil halt weil halt einfach was passiert ist. Und ich frage mich halt jetzt, was was äh womit müssen wir jetzt rechnen?\nMan kann sich sicherlich eine Menge Gutes und eine Menge äh Furchtbares vorstellen. Äh wir haben's ja wahrscheinlich mit einer klassischen Do-Use-Technologie zu tun."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:23:14",
      "text": "Auf jeden Fall, ich meine stell dir vor ich stelle's mir gerne so vor, mhm,\nDie Computer sind in der Lage, einfache Probleme zu lösen, ähm die sie vorher überhaupt nicht lösen konnten, über den man nicht mal nachgedacht hat wie Bilderkennung, Strukturen in Texten ermitteln. Das sind so klassische,\nIch will niemanden beleidigen, der in einem Büro arbeitet, aber äh viele Bürotätigkeiten sind genauso scanne auf eine geschickte Art und Weise die Akten durch und lege sie in neue Haufen.\nUnd das ging bisher nicht, weil der Computer weder den Text gut erkennen konnte, noch den Content verorten. Ähm ich prophezeie mal, dass das in den nächsten zwei, drei Jahren sehr gut funktionieren wird und ähm,\nBeim selbstfahrenden Auto ist jedem klar, dass äh was sich in der Welt verändern wird an an Lieferindustrie und äh Fahrern und äh,\nAber bei diesem ganzen die Computer sind einen entscheidenden Schritt schlauer geworden. Das wird auch ganz schön was hinter sich herziehen an jetzt am reinen Arbeitstechnischen äh."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:24:18",
      "text": "Steuererklärung überprüfen. Steuererklärung machen wir toll."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:24:24",
      "text": "Komponieren."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:24:25",
      "text": "Ja, ich würde ja ganz gerne einfach so, wenn ich so alle meine Belege scanne und ein neuroundales Netzwerk damit trainiere und am Ende macht er mir mal eine Steuererklärung."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:24:35",
      "text": "Das wäre großartig."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:24:36",
      "text": "Hammer, die wir auch mal richtig,\nMan hätte nichts ja also bis hin zu der Entscheidung, ob er irgendwie einen Beleg jetzt überhaupt relevant ist. Und ich meine,\noder ohne Witz, also ich äh überlege jetzt die ganze Zeit schon äh parallel was eigentlich solche Bereiche sind, wo,\nein umfassendes Verständnis auch noch des letzten Details eine sehr hohe Relevanz hat, was heute irgendwie äh weggeht, ja? Ich meine, wenn ich das jetzt mal zum Beispiel auf Gesetze mache,\nSteuergesetze? Also wenn ich sozusagen das Gesetzeswerk an sich parse, dann weiß das System, wo die Lücken sind."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:25:18",
      "text": "Ja, ich glaube, so weit sind wir noch nicht, aber das ist absolut denkbar, dass man in der Richtung forschen kann. Ne?"
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:25:23",
      "text": "Ja ist es also ich meine allein so eine Frage ist es legal das und das zu tun."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:25:28",
      "text": "Ich glaube du brauchst."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:25:32",
      "text": "Widersprüche finden."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:25:33",
      "text": "Widersprüche finden ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:25:34",
      "text": "Also so jetzt mal ein positives Beispiel äh Gesetzgebung, ja, widerspricht ist mein Gesamtregelwerk durch diese Änderung äh geht es in Widersprüche oder ist überhaupt schon das existierende äh äh,\nGesetzeswerk steht es zu sich im Widerspruch, was es unter Garantie an vielen Stellen steht. Die Frage ist nur an welchen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:25:54",
      "text": "Ist es wieder schmutzfrei, ja,\nOkay, dann kommen wir sofort an so äh Widerspruchsfreie also an künstliche und echte Grammatiken und das äh füllt noch mal mehrere Podcasts, glaube ich, dann können wir gleich auch wieder den die Brücke zu und ähnlichem Schlagen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:26:10",
      "text": "Podcasts sind wahrscheinlich am Ende die einzige Technologie, die überleben wird, weil wir müssen darüber reden."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:26:17",
      "text": "Tatsächlich habe ich mit meinem äh Ex-Kollegen neulich war ich Chinesisch essen und äh der Chinese war neu und ähm,\nIch weiß nicht, wie das ist, wenn man so ein Set-up für ein chinesisches Restaurant macht oder das Essen war hervorragend, aber die Musik war zum Davonlaufen. Das war so ein,\nRichard Kleidermann Mut zack durch irgendeinen schrecklichen Sintisizer Filter gesetzt. Das war aber garantiert gema-frei und ich denke\nDas war der Grund, warum Sie's angeschafft haben. Irgendwer kam an und hat gesagt, wenn ihr ein chinesisches Restaurant macht, braucht ihr Musik. Hier ist eine gemafreie CD. Die kostet so und so viel, die dürft ihr spielen.\nDie GEMA kostet entsprechend mehr,\nUnd es war wirklich so, es war zu, es war zu erbärmlich. Also wir haben beide sehr gelitten und ich habe dann gesagt, okay, es muss doch das ist jetzt eine Idee für ein Geschäftsmodell, also wenn da draußen jemand zuhört, ich äh gebe das gerne äh ab.\nMan muss doch in der Lage sein, man müsste doch in der Lage sein, ein neuronales Netzwerk da drauf zu trainieren, on the fly vernünftige Musik zu spielen und die ist dann garantiert gema-frei, weil die ist in dem Augenblick erfunden. Und diese kleinen Chips."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:27:15",
      "text": "Komponieren."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:27:16",
      "text": "Zu zu komponieren und den auf einem Raspberry whatever äh direkt ausliefert für 50 Euro und dann haben wir."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:27:25",
      "text": "Also wenn die Musik dann aber so klingt, wie diese Google Dreams aussehen, dann."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:27:30",
      "text": "Ja ich weiß."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:27:31",
      "text": "So wenn nicht sogar creepy irgendwie. Ja das war ja auch so mein erster äh Scherzname für Siri.\nOkay, Musik komponieren, Musik erkennen natürlich auch, ne? Das ist natürlich äh ähnliche Sache, Muster äh erkennen. Klar, das ist bei Musik voll drin.\nHm"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:27:55",
      "text": "Was es auch noch braucht, ist eine Vogelstimm-Erken-App. Ich glaube, das hat sich jetzt was getan, aber es war ein Lieblingsprojekt meiner der letzten drei Jahre von mir, zu überlegen, es muss doch möglich sein,\nMikrofone sind sehr blöd, sind nicht sind gut, aber man kommt mit so einem Mobiltelefon nicht nah genug an den Vogel ran, um den Vogelgesang,\nweitzuschalten, aber tatsächlich äh wenn man mit Kindern im Wald unterwegs ist, dann stellt man, kriegt man oft die Frage gestellt und was war das für ein Vogel? Und ich habe gedacht, das muss doch möglich sein, zum Netzwerk zu trainieren, dass ich in meinem Smartphone sage, Spatz.\nOder ebent. Dohle."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:28:32",
      "text": "Äh du wie du willst das sagen? Du willst den du will."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:28:34",
      "text": "Ich will ich will ich will."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:28:35",
      "text": "Vogel aufnehmen und willst gesagt bekommen wir äh äh uns beide wahrscheinlich nicht nur einen, sondern äh da äh ist jetzt ein Spatz, der von einer Amsel unterbrochen wird über die sich äh okay."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:28:46",
      "text": "Beschwert."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:28:47",
      "text": "Ja.\nOkay, das wäre vorstellbar. Mir würde jetzt Pilz äh Bestimmungen einfallen. Ich hatte grad das Problem im Wald, dass\nMan so als äh Naturliga Selika natürlich dann so überhaupt nicht weiß, womit man's zu tun hat. Mit schmeckt das und ist das äh vor allem giftig? Äh und die Unterscheidungsmerkmale visuell."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:29:09",
      "text": "Das ist das wäre eine klassische Anwendung für ein für ein visuelles neuronales Netzwerk. Trainierst auf Pilze, also nimm keine Ahnung sind die Anzahl der Speise- und Giftpilze in Deutschland ist endlich."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:29:19",
      "text": "Man muss wahrscheinlich noch nicht mal viel reintun, sondern in der Wikipedia sind die Fotos von den Pilzen und die Informationen, ob sie giftig sind und äh dann brauche ich nur noch genug YouTube-Videos."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:29:33",
      "text": "Von den Pilzen."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:29:34",
      "text": "Von den Pilzen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:29:35",
      "text": "Weil ich muss den Pilz aus allen möglichen Blickwinkeln in jedem Lichtverhältnis haben, weil dieses Setup in das in das äh Netzwerk rein muss und dann erkennt es jeden Pilz bei jedem Licht, ja."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:29:44",
      "text": "Und dann kann ich mein Telefon nehmen und einfach die Kamera einschalten und da so bisschen drum herum gehen und dann sagt er, ah ja, hier Knollenblätterpilz. Lass mal lieber oder Steinpilz lecker."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:29:57",
      "text": "Ganz hübsch oder?"
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:29:59",
      "text": "Was ist mit unserer Kommunikation? Also jetzt haben wir ja gerade hier so wieder mal schön eine Welle mit äh Hater, Kommentaren, Nazis im Netz, äh,\nund wir waren ja ganz am Anfang schon beim Spam."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:30:16",
      "text": "So was kann man super erkennen. Das ist äh mehr als als hervorragend zu erkennen. Du kannst ähm,\nDas sind die recurrend Earl Networks, die RNNs, die ähm sind,\nin sich selbst zurückgekoppelt. Das heißt, sie haben äh wie können sich an bestimmte Subsequenzen erinnern und mit denen äh kann man,\nSetups trainieren, dass man einen Computer dazu befähigt, wie Shakespeare zu schreiben oder wie Nazi zu blöken,\nOder zu schreiben,\nÄhm es gibt ein sehr, sehr schönes Projekt. Da hat einfach jemand den gesamten Linus-Körner-Code in so einen Network als Trainingsset-up reingebaut und sagt dann danach, okay und jetzt schreibe mir einen neuen Code und da kommt C-Code bei Raut bei raus.\nDer ist manchmal sogar kompilierbar. Der macht nichts Sinnvolles, aber äh erstaunlich.\nEr sieht so aus wie und oder er du kannst äh alle alle Tech-Texte aus dem AKFX nehmen äh zur,\nKeine Ahnung, Hochenergiephysik und am Ende sagen, generiere mir einen neuen Text und es kommt tatsächlich ein Paper raus, was wo man schon zweimal hingucken muss, bis man feststellt, okay, das ist kompletter Bullshit, aber die Struktur stimmt total."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:31:29",
      "text": "Oder Hausaufgaben. Auweia."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:31:30",
      "text": "Hausaufgaben, großes Thema. Da kommt was auf uns zu.\nAlso bisschen was wird sich verändern tatsächlich. Äh das sind wie gesagt das das Training, es dauert lange und es ist aufwendig, aber am Ende kommt was dabei raus, was aus einem Smartphone,\nwas macht, was früher einfach mal eine Quell YMP war, so ne."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:31:55",
      "text": "Und die Prozessing Power ist da teilweise lokal, teilweise auf jeden Fall äh im Netz."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:32:02",
      "text": "Na oder sie ist, das haben wir ganz vergessen zu sagen, einer der größten oder der,\nEntscheidenden Gründe, warum das so gut funktioniert mit den neuronalen Netzen, sind tatsächlich die Gamergrafikkarten, weil neuronale Netze zu trainieren sind unglaublich viele Schritte, wo man Matrizen multiplizieren muss und ähm,\nDas können normale Computer können das schon, aber die sind überhaupt nicht dafür optimiert und die machen das auch nicht parallel.\nJedenfalls nicht im Normalfall, ähm aber Grafikkarten müssen genau dieses Problem, viele Millionen Mal pro Sekunde lösen, die können das hervorragend. Die können nur das, aber das können sie super schnell und deswegen werden,\nneuronale Netze gerne auf äh so Invidia Grafikkarten trainiert. Das geht das geht schön schnell."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:32:48",
      "text": "Beziehungsweise die neuen Architekturen, die halt ja im Prinzip von vornherein schon die diese,\nWas man bisher halt GPU genannt hat, jetzt allgemein eben so als Number Cruncher verwendet, Open CL und vergleichbare Standards haben da ja äh im Prinzip die Zugänglichkeit auch schon\nlängst geschaffen. Jetzt ist es nur noch eine Frage, auch Software zu finden, die daraus einen Nutzen ziehen kann und du sagst, neurale Netzwerke\nsind genau das, was davon äh wunderbar profitieren kann. Das heißt, mit der weiterhin ähm stark zunehmenden Gesamtperformance, gerade auch von Smartphones im grafischen Bereich.\nSteigt auch die Verarbeitungsgeschwindigkeit der äh Telefone an der Stelle."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:33:26",
      "text": "Auf jeden Fall."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:33:30",
      "text": "Ja, ich meine, das hat wirklich äh eine eine äh ich glaube jedem, der jetzt hier gerade zuhört, summt äh jetzt schon so ein bisschen das Hirn, weil äh die Implikationen sind natürlich,\nso vielfältig, dass dass wir hier nicht mal auch nur ansatzweise daran kratzen können, glaube ich, ähm worauf es alles eine eine Auswirkung haben kann im Guten wie im Bösen, ne? Wenn ich noch mal das Facebook Beispiel rausgreifen kann,\nmit dem was wir jetzt sozusagen an an äh was wir hier so erarbeitet und diskutiert haben.\nWir haben da einen Ort,\nder von sich aus schon so wohl definiert und so Metadaten reich ist, dass dass dass man ja noch nicht mal bei null anfangen muss. Wo sozusagen schon so unglaublich viel Struktur da ist und sagen wir mal auch für\nFür Menschen und natürlich allen voran den Betreibern, aber nicht nur den äh sozusagen klar ist, was das alles bedeutet.\nAber eben unfassbare Text, Bild, Videomengen, die in einem unmittelbaren Zusammenhang stehen und die sich im Prinzip mit so einem neuralen Netzwerk,\nwunderbar äh bearbeiten lassen und,\nwie man nur vermuten kann, vermutlich auch bearbeitet werden. Da das eben die Basis ist für eine Bildersuche, für eine Textsuche, für eine Relevanzrechnung, um nur eine Anzeige anzuzeigen, aber die natürlich als Nebenprodukt,\nAuch noch ganz andere äh Ableitungen ermöglicht, weil man ja eigentlich alles versteht. Also man versteht den,\nAlso lenke ein, wenn ich jetzt ein bisschen übers Ziel hinausschieße, aber im Prinzip wird ja hier auch ein Verständnis des Diskurses. Wer redet mit wem worüber?\nUnd warum? Scheint mir jetzt nicht so ein unerreichbares Ziel zu sein, wenn man's darauf auslegen würde."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:35:20",
      "text": "Nee, das ist gut machbar. Also das äh da muss man ein bisschen Gehirnschmalz drauf werfen, aber das ist mit Sicherheit lösbar und das wird auch getan an vielen Stellen, die man, die jetzt sofort auf der Hand liegen, ne?\nAlso warum jeder, der sich damit beschäftigt wird die naheliegendste Technologie nehmen und das ist wäre die naheliegendste Technologie. An der Stelle jedenfalls die, die am besten funktioniert.\nIst jetzt wahrscheinlich eine Frage der Implementierung, äh ob man das einfach äh auf dem entsprechenden Rechner sofort anwenden darf oder ob's da Restriktionen gesetzlicher Art gibt, aber gemacht wird das sowieso, weil alles gemacht wird, was geht,\nWas gut geht, wird sogar lieber gemacht."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:35:59",
      "text": "Und das geht jetzt ganz gut."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:36:01",
      "text": "Das geht jetzt ganz gut. Das klingt alles so ein bisschen, als wenn das alles ganz problemlos wäre, ne? Also ähm in meinem täglichen Arbeiten und auch in dem meiner Kollegen ist es nach wie vor so, dass wir,\nUns das Problem äh sehr genau angucken, müssen mit ihren Features äh mit den mit den,\nMit der Art und Weise, wie wir die rausarbeiten, damit wir äh,\nDie Kundenantworten gut beantworten können, dass wir nehmen, wir greifen nicht einfach pauschal in den Schrank, wo drauf steht, ah, neuronale Netzwerke und äh das ist das Problem vom Kunden XY, das."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:36:34",
      "text": "Das Modul ein und dann."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:36:35",
      "text": "Und fertig. So nee nee."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:36:36",
      "text": "Die sie schon immer haben wollten."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:36:38",
      "text": "Da ist schon ganz normale Statistik äh nach nach wie vor dabei."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:36:42",
      "text": "Ja. Na klar, einfach um auch dieses Problem, klar hat das irgendwie alles erlernt, aber die Kunst ist ja letzten Endes dann äh herauszufinden, wo wo liegt denn das jetzt Erlernte und wie frage ich's ab?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:36:53",
      "text": "Genau. Und die Art und Weise, wie die Neuronen aufgebaut werden, äh ob ich sie in rekursiv miteinander verkette, wie viele Ebenen, wie viele, das ist äh schon auch noch eine ganze Menge ausprobieren. Das Ausprobierarbeit."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:37:10",
      "text": "Wie viel Fail ist denn jetzt noch in dem derzeitigen Ansatz?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:37:14",
      "text": "Louis. Das ist."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:37:16",
      "text": "Wenn es früher sozusagen an einer bestimmten Stelle äh endete und jetzt durch die Arbeiten von hinten so einen Durchbruch erzielt hat, dass es jetzt an vielen Stellen gut ist.\nWo ist die Garantie, dass es jetzt perfekt ist?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:37:32",
      "text": "Also ich glaube, es ist viel besser verstanden. Ähm es macht auch nicht mehr so, es hat nicht mehr diesen diesen Moment. Es gibt diesen Moment nicht mehr, dass es irakische Antworten gibt, was man nicht haben möchte, wenn man sich mit Maschinen unterhält.\nÄh das ist relativ gut verstanden. Hm wo das jetzt überall hinführt und was man noch alles drauf werfen kann, da stehen wir tatsächlich am Anfang. Also,\nDu kannst dir so eine Google-Plus-Maschine Learning-Gruppe nehmen und äh da jeden Tag drauf gucken und da ist jeden Tag wirklich eine komplett neue Idee drin. Das ist nicht so, dass man denkt, ach komm jetzt hat er es schon letzte Woche gesagt und das kennen wir schon vom letzten Jahr, sondern Leute werfen,\nSagen, wir brauchen wir wollen ein neuronales Netzwerk bauen, um äh einen Schach Computer zu bauen. Ist jetzt nichts, was so ganz doll ist, aber Schach ist äh ist eine alte Anwendung. Ähm,\nAus dem Computer.\nEs ist unglaublich viel äh Rechenpower und Menschenpower in diese Schachprogrammierung reingeflossen, so man denke an IBM Deep Blue, ne?\nIrgendwelche Jungs haben,\nIn den letzten Monaten ein ganz einfaches Schachprogramm genommen mit einer freien mit einer freien Zuge. Ist nicht viel, nicht viel äh Schachwissen drin, wirklich nicht.\nUnd haben dann ein Deep-Learning-Netzwerk dran äh geknüppert und haben als Trainingsset alle online zur Verfügung stehenden Schach äh,\nPartien drauf angewendet und tatsächlich äh nach wenigen Monaten Training spielt das Ding auf Großmeisterniveau jetzt. Das ist,\ndas ist was, ne? Ein Großmeister, der fällt jetzt nicht einfach so aus irgendeinem Hi."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:39:10",
      "text": "Himmel, ja. Aha. Krass. Ähm okay.\nGut Schach ist ja schon schon länger so ein bisschen oft auf auf dem Level, wo die Menschen nur noch äh eingeschränkt mithalten können. Aber das ist dann sozusagen jetzt auch schon eine,\nÄhm eine Leistung, die in also,\nnur als Beispiel, weißt du wie groß diese Datenmenge ist, die an der Stelle dann generiert wird, also wie, wie, wie, wie umfassend muss man sich diese diesen Datenbestand,\nVorstellen, den man dann lokal noch braucht, um diese Schachintelligenz auch ohne jetzt ein komplettes Google dahinter noch äh zum Einsatz zu bringen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:39:51",
      "text": "Das weiß ich nicht. Beim Schwalmschach weiß ich nicht, aber wenn du."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:39:54",
      "text": "So eine Ahnung."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:39:55",
      "text": "Nee, auch nicht, ist, ich denke, ein paar Megabyte."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:39:57",
      "text": "Megaarbeit. Keine Tera."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:39:58",
      "text": "Ja ne sicher keine Tier."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:40:01",
      "text": "Also passt in jedes Telefon in eine App. Und da ist dann einfach mal das Wissen über Schacht TM drin."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:40:09",
      "text": "Also,\nKeine Ahnung, das Ding wird ein paar hunderttausend Neuronen haben, schätze ich jetzt einfach mal. Ich weiß es nicht. Das das Paper dazu, das das Biest heißt Giraffe, das kann man jetzt selber googeln. Ich weiß nicht, weiß wirklich nicht, wie viele. Äh und jetzt hat\nJedes Neuronen, keine Ahnung, 15 Verbindungen, dann sind es so viele Neuronen, mal so viele Verbindungen an Koeffizienten, die ich aufheben muss. Das das was das neuronale Netzwerk ausmacht.\nWas kann ich äh noch ordentlich zusammenzippen? Also das kann ich komprimieren und dann sind's wahrscheinlich nur ein paar Megabyte,\nirre. Ja genau."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:40:40",
      "text": "Gary. Okay. Wirklich mal echt wohl selektierte Bits."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:40:48",
      "text": "Also das ist äh ganz in Ruhe optimiert."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:40:56",
      "text": "Hm tja was können wir denn,\nan äh positiver äh Vision jetzt hier am Ende noch. Mir fällt jetzt so viel Dystopie ein.\nIch will jetzt hier gar nicht alle äh komplett äh runterfahren. Ich meine, gut so autonomer Verkehr und so weiter. Das haben wir alles schon gemacht, aber es ist natürlich schon ein,\nProblem, dass wir ähm im Prinzip jetzt so vor der Zeit stehen, wo einfach,\nUnsere ureigene, menschliche, inhaltliche Kommunikation,\nsich vollständig analysieren äh äh lässt und das eben bis in alle Ebenen hinein, weil einfach\nja im Prinzip die Abstraktionsebene soweit getrieben werden kann mit diesen Deep-Learning Netzwerks, dass sie vielleicht am Ende auch unsere,\nAbstraktionsebenen, die wir so zu leisten in der Lage sind, noch einfach übersteigen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:41:48",
      "text": "Ja, also da sind wir jetzt ganz schnell an diesen ganzen Singularitätsgeschichten, ne?"
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:41:52",
      "text": "Ja, was ist denn, wenn man jetzt irgendwie alle wissenschaftlichen äh Papers äh in das Deep-Learning-Netzwerk reingibt, haben wir dann die äh Universaltheorie für zwischen Einstein und Eisenberg verheiratet?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:42:05",
      "text": "Nee, ich glaube nicht. Also wir haben danach äh die Struktur aller mathematischen Texte gelernt, die Struktur und können wie ein Idiot ähnliche Texte schreiben, die aber,\nkeine Semantik haben, also da wir sind noch nicht bei der Semantik und ich glaube, das braucht auch noch eine Weile, bis wir da landen. Im Augenblick sind wir bei der Syntax. Wir verstehen die Syntax und die Syntax von Bildern und äh,\nbestimmten anderen Dingen verstehen wir einfache Strukturen und einfache Handlungsanweisungen wie ein Schachspiel oder ein Go Spiel verstehen wir auch,\nUnd können plötzlich die Komplexität radikal verflachen. So das ist neu, aber Selbstbewusstsein,\nIch glaube noch nicht. Das dauert noch eine Weile."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:42:48",
      "text": "Aber die Antwort auf die Frage mit aus welchem Grund wurde mir diese Mail geschickt und ist da Spam?"
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:42:58",
      "text": "Die ist gut zu beantworten. Ich weiß nicht, wer äh wahrscheinlich hat jeder Mensch auf diesem Planeten einen Google-Mail-Account und ähm da sieht man, wie gut so was funktioniert, ne. Also der Google Spam-Filter,\nKeine Ahnung, ich kriege einmal alle drei Monate kriege ich eine Spammail in meinen in meiner Inbox, ansonsten landen die alle im Spam-Filter,\nIch weiß nicht, ob das rein ist, äh Collabrito Filtering ist oder ob da auch ein neuronales Netzwerk hinter steckt, das weiß ich tatsächlich nicht, aber ich."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:43:25",
      "text": "Aber wenn's das jetzt noch nicht tut, dann ist es durchaus absehbar, dass es dann irgendwann mal so sein wird. Weil es ja eigentlich ein klassisches äh Problem ist auch mit unmittelbaren finanziellen,\npositiven Wirkung, also zumindest für so ein Unternehmen, was das betreibt."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:43:42",
      "text": "Ja und stell dir vor, du hast diesen neuronalen Filter und der ist auch nur wenige Megabyte groß, vielleicht setzt man ihn einfach auf alle Core-Router im Internet an, auf alle,\nUnd dann kann man den Spam, muss man ihn gar nicht mehr um den halben Globus herumrouten, sondern hältst ihn schön lokal da, wo er auftritt und."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:43:56",
      "text": "Löst das Problem an der Stelle, wo es entsteht,\nJa gut und dann ist halt die Frage, was tut dieser Spam-Filter eigentlich wirklich? Haben sie dieses Netzwerk jetzt selbst trainiert äh oder sind diese Datei nur zugestellt worden?\nWeiß der Geier, was für Informationen an der Stelle würde ich raus machen. Also da haben wir wirklich auf auf so vielen Ebenen auch ein neues äh philosophisches Problem.\nMensch, Ulf. Ich glaube an der Stelle."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:44:23",
      "text": "Hören wir auf? Okay, das wird sonst auch einfach ein bisschen viel. Es kommt mir schon so vor. Ich."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:44:29",
      "text": "Ist auch viel, aber es äh so ist das hier bei CAE und so soll es äh auch sein und äh ich bedanke mich,\nFür die Ausführung hier zu den neuronalen Netzwerken und ich hoffe, ihr könnt jetzt alle ruhig schlafen."
    },
    {
      "speaker": "Ulf Schöneberg",
      "time": "2:44:43",
      "text": "Das hoffe ich auch."
    },
    {
      "speaker": "Tim Pritlove",
      "time": "2:44:46",
      "text": "Aber äh eines Tages werdet ihr auch wieder aufwachen und dann habe ich vielleicht auch schon die nächste Sendung für euch parat. Wenn's sehen. Gut, ich sage äh tschüss äh und bis bald."
    }
  ]
}