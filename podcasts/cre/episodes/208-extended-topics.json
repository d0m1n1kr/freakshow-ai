{
  "episodeNumber": 208,
  "title": "CRE208 Neuronale Netze",
  "extractedAt": "2026-01-02T23:29:13.965Z",
  "dataSource": "description",
  "topics": [
    {
      "topic": "Durchbrüche in der Künstlichen Intelligenz",
      "subject": {
        "coarse": "Technology",
        "fine": "Artificial Intelligence"
      },
      "summary": "In der Episode wird über Fortschritte in der Künstlichen Intelligenz, insbesondere in Bezug auf neuronale Netze, diskutiert. Ulf Schöneberg erklärt, dass die Struktur und das Design von Neuronen entscheidend sind und viel Experimentierarbeit erfordern. Ein Beispiel für den Fortschritt ist ein einfaches Schachprogramm, das durch Deep Learning und Training mit Online-Partien auf Großmeisterniveau spielt. Die benötigte Datenmenge für solche Anwendungen ist relativ gering, sodass sie auf mobilen Geräten implementiert werden kann. Das Projekt, das erwähnt wird, heißt 'Giraffe'.",
      "summaryMeta": {
        "transcriptFound": true,
        "transcriptFile": "208-ts.json",
        "method": "keyword-window",
        "pickedCount": 17,
        "startSec": 9376,
        "endSec": 9660,
        "generatedAt": "2026-01-03T00:02:11.372Z",
        "llm": {
          "provider": "openai",
          "model": "gpt-4o-mini",
          "temperature": 0.3
        }
      }
    },
    {
      "topic": "Funktion von neuronalen Netzen",
      "subject": {
        "coarse": "Technology",
        "fine": "Neural Networks"
      },
      "summary": "In diesem Transkript-Ausschnitt diskutieren Ulf Schöneberg und Tim Pritlove die Entwicklung und Anwendung neuronaler Netze in den 1980er Jahren. Schöneberg beschreibt, dass neuronale Netze zu dieser Zeit als Teil des maschinellen Lernens aufkamen, als die Rechenleistung der Computer zunahm. Ein bekanntes Beispiel für maschinelles Lernen ist der MNIST-Datensatz, der gescannte Zahlen enthält und als 'Hello World' des visuellen maschinellen Lernens gilt. Die Herausforderung besteht darin, diese gescannten Zahlen korrekt zu erkennen und zu klassifizieren.",
      "summaryMeta": {
        "transcriptFound": true,
        "transcriptFile": "208-ts.json",
        "method": "keyword-window",
        "pickedCount": 42,
        "startSec": 1287,
        "endSec": 4719,
        "generatedAt": "2026-01-03T00:02:13.190Z",
        "llm": {
          "provider": "openai",
          "model": "gpt-4o-mini",
          "temperature": 0.3
        }
      }
    },
    {
      "topic": "Anwendungen von neuronalen Netzen",
      "subject": {
        "coarse": "Technology",
        "fine": "Applications"
      },
      "summary": "In dieser Episode wird die Entwicklung und Anwendung neuronaler Netze thematisiert. Tim Pritlove und Ulf Schöneberg diskutieren die Anfänge der neuronalen Netze in den 1980er Jahren, als die Rechenleistung der Computer es ermöglichte, komplexe Probleme zu bearbeiten. Ein Beispiel ist der Amnest-Datensatz, der zur Erkennung handgeschriebener Ziffern verwendet wurde. Die Herausforderung besteht darin, Variationen in der Schrift zu erkennen, was neuronale Netze durch Datenreduktion und Mustererkennung leisten. Die Diskussion berührt auch die Verbindung zwischen neuronalen Netzen und den kognitiven Prozessen im menschlichen Gehirn.",
      "summaryMeta": {
        "transcriptFound": true,
        "transcriptFile": "208-ts.json",
        "method": "keyword-window",
        "pickedCount": 52,
        "startSec": 4245,
        "endSec": 5584,
        "generatedAt": "2026-01-03T00:02:18.979Z",
        "llm": {
          "provider": "openai",
          "model": "gpt-4o-mini",
          "temperature": 0.3
        }
      }
    }
  ],
  "extendedAt": "2026-01-03T00:02:18.979Z",
  "transcriptFound": true,
  "transcriptFile": "208-ts.json"
}
